{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d84d490a",
   "metadata": {},
   "source": [
    "### Best Model and Imbalance Handling\n",
    "#### From `notebooks\\3_model_exploration.ipynb`\n",
    "- SMOTE + XGBoost\n",
    "\n",
    "### Steps\n",
    "1. reproduce the feature engineering again in each K-fold\n",
    "2. train and preprocess properly to ensure no data leakage\n",
    "3. evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd88b8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\alden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\alden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# NLP preprocessing\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "import wordninja\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "TEXT_COLS = ['title', 'description', 'requirements', 'benefits', 'company_profile']\n",
    "CATEGORICAL_COLS = ['employment_type', 'required_experience', 'required_education', 'industry', 'function', 'location', 'department']\n",
    "BINARY_COLS = ['telecommuting', 'has_company_logo', 'has_questions', 'fraudulent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c9c35",
   "metadata": {},
   "outputs": [],
   "source": "def handle_nulls_upfront(df):\n    \"\"\"\n    Handle all null values upfront before any processing.\n    This ensures consistent null handling throughout the pipeline.\n    \n    Strategy:\n    - Text columns → empty string \"\"\n    - Categorical columns → \"unknown\"\n    - salary_range → \"unknown\"\n    - Binary columns → keep as-is (will be handled during model training)\n    \"\"\"\n    df_clean = df.copy()\n    \n    # Text columns: replace with empty string\n    for col in TEXT_COLS:\n        if col in df_clean.columns:\n            df_clean[col] = df_clean[col].fillna('').astype(str)\n            # Also handle explicit null-like strings\n            df_clean[col] = df_clean[col].replace(['nan', 'NaN', 'None', 'none'], '')\n    \n    # Categorical columns: replace with 'unknown'\n    for col in CATEGORICAL_COLS:\n        if col in df_clean.columns:\n            df_clean[col] = df_clean[col].fillna('unknown').astype(str)\n            # Handle various null-like values\n            null_like = ['Not Applicable', 'NaN', 'not applicable', 'Unspecified', \n                        'Other', 'Others', 'none', 'na', 'n/a', '', ' ', 'nan', 'None']\n            df_clean[col] = df_clean[col].replace(null_like, 'unknown')\n    \n    # Salary range: replace with 'unknown'\n    if 'salary_range' in df_clean.columns:\n        df_clean['salary_range'] = df_clean['salary_range'].fillna('unknown').astype(str)\n    \n    return df_clean\n\n\ndef validate_nulls(df, stage_name=\"\"):\n    \"\"\"\n    Validate that no unexpected nulls exist after processing.\n    Reports null counts and returns boolean indicating if nulls were found.\n    \"\"\"\n    null_counts = df.isnull().sum()\n    has_nulls = null_counts.any()\n    \n    if has_nulls:\n        print(f\"Nulls found after {stage_name}:\")\n        print(null_counts[null_counts > 0])\n        return False\n    else:\n        print(f\"No nulls found after {stage_name}\")\n        return True\n    \ndef cleanAndDeduplicate(df):\n    df_cleaning = df.copy()\n\n    def simplify_employment_type(x):\n        if not isinstance(x, str) or x == 'unknown':\n            return 'unknown'\n        \n        x = x.strip().lower()\n        if x in ['full-time', 'part-time']:\n            return x\n        elif x in ['contract', 'temporary']:\n            return 'non-permanent'\n        else:\n            return 'unknown'\n    \n    df_cleaning['employment_type_clean'] = df_cleaning['employment_type'].apply(simplify_employment_type)\n\n    def comparison_key(row):\n        emp = None if row['employment_type_clean'] == 'unknown' else row['employment_type_clean']\n        return (row['location'], row['title'], row['description'], row['requirements'], emp)\n\n    df_cleaning['dedup_key'] = df_cleaning.apply(comparison_key, axis=1)\n    df_deduped = df_cleaning.drop_duplicates(subset=['dedup_key'])\n    \n    print(f\"Removed {len(df_cleaning) - len(df_deduped)} duplicate rows\")\n    return df_deduped\n\ndef check_corpus(df, text_cols):\n    corpus_stats = {}\n\n    for col in text_cols:\n        texts = df[col].fillna(\"\").astype(str).str.lower().tolist()\n\n        tokens = []\n        for t in texts:\n            tokens.extend([w for w in word_tokenize(t) if len(w) > 2])\n\n        corpus_stats[col] = len(set(tokens))\n\n    print(corpus_stats)\n    return corpus_stats\n\ndef apply_text_normalization(df, text_cols):\n    \"\"\"\n    Normalize text: lowercase, remove URLs, punctuation, extra whitespace.\n    \n    Note: Assumes nulls have been handled - all text should be strings.\n    Empty strings remain empty and are handled properly.\n    \"\"\"\n    def normalize_text(text: str) -> str:\n        # Handle empty strings (from nulls)\n        if not text or not text.strip():\n            return \"\"\n\n        text = text.lower().strip()\n        text = re.sub(r\"http\\S+|www\\S+\", \" \", text)  # remove URLs\n        text = re.sub(r\"[^a-z\\s']\", \" \", text)  # remove punctuation/numbers except apostrophes\n        text = re.sub(r\"\\s+\", \" \", text).strip()\n\n        return text\n    \n    for col in text_cols:\n        tqdm.pandas(desc=f\"Normalizing {col}\")\n        df[col] = df[col].progress_apply(normalize_text)\n    \n    print(\"Text normalization complete\")\n    return df\n\n\ndef apply_split_df(df, text_cols):\n    \"\"\"\n    Split CamelCase and joined words using wordninja.\n    Examples: 'SmartContract' -> 'Smart Contract', 'makemoney' -> 'make money'\n    \"\"\"\n    def split_camel_case(token):\n        \"\"\"Splits CamelCase tokens: 'SmartContract' -> ['Smart', 'Contract']\"\"\"\n        return re.sub('([a-z])([A-Z])', r'\\1 \\2', token).split()\n\n    def split_joined_words(text, min_len=10):\n        # Handle empty strings\n        if not text or not text.strip():\n            return \"\"\n        \n        tokens = text.split()\n        new_tokens = []\n\n        for token in tokens:\n            # Skip short tokens\n            if len(token) < min_len:\n                new_tokens.append(token)\n                continue\n\n            # 1. Try CamelCase split\n            camel_split = split_camel_case(token)\n\n            if len(camel_split) > 1:\n                # After splitting CamelCase, apply wordninja to each part\n                final_parts = []\n                for part in camel_split:\n                    wn = wordninja.split(part)\n                    final_parts.extend(wn)\n                new_tokens.extend(final_parts)\n                continue\n\n            # 2. If no CamelCase, try wordninja directly\n            wn = wordninja.split(token)\n            if len(wn) > 1:\n                new_tokens.extend(wn)\n            else:\n                new_tokens.append(token)\n\n        return \" \".join(new_tokens)\n\n    for col in text_cols:\n        tqdm.pandas(desc=f\"Splitting joined words in {col}\")\n        df[col] = df[col].progress_apply(split_joined_words)\n    \n    print(\"Word splitting complete\")\n    return df\n\ndef remove_stopwords_df(df, text_cols):\n    \"\"\"\n    Remove English stopwords from multiple text columns.\n    Handles empty strings gracefully.\n    \"\"\"\n    def remove_stopwords_text(text):\n        # Handle empty strings\n        if not text or not text.strip():\n            return \"\"\n        \n        tokens = word_tokenize(text.lower())\n        clean_tokens = [t for t in tokens if t not in STOPWORDS and len(t) > 2]\n        return \" \".join(clean_tokens)\n    \n    for col in text_cols:\n        tqdm.pandas(desc=f\"Removing stopwords in {col}\")\n        df[col] = df[col].progress_apply(remove_stopwords_text)\n    \n    print(\"Stopword removal complete\")\n    return df\n\ndef lemmatize_df(df, text_cols):\n    \"\"\"\n    Lemmatize multiple text columns.\n    Handles empty strings gracefully.\n    \"\"\"\n    def lemmatize_text(text):\n        # Handle empty strings\n        if not text or not text.strip():\n            return \"\"\n        \n        tokens = word_tokenize(text.lower())\n        lemmas = [lemmatizer.lemmatize(t) for t in tokens if len(t) > 2]\n        return \" \".join(lemmas)\n    \n    for col in text_cols:\n        tqdm.pandas(desc=f\"Lemmatizing {col}\")\n        df[col] = df[col].progress_apply(lemmatize_text)\n    \n    print(\"Lemmatization complete\")\n    return df\n\ndef addWordPatterns(df):\n    patterns = {\n        'urls': r'http[s]?://\\S+',\n        'emails': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n        'phone_numbers': r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b',\n        'money_symbols': r'[$€£¥]',\n        'other_symbols': r'[©®™]',\n    }\n    \n    def count_patterns(text, patterns):\n        if not text or not text.strip():\n            return {k: 0 for k in patterns.keys()}\n        return {k: len(re.findall(pat, str(text))) for k, pat in patterns.items()}\n\n    for col in ['description', 'company_profile', 'requirements', 'benefits']:\n        if col in df.columns:\n            feat_df = df[col].apply(lambda x: count_patterns(x, patterns))\n            feat_df = pd.DataFrame(list(feat_df), index=df.index).add_prefix(f'{col}_')\n            df = pd.concat([df, feat_df], axis=1)\n\n    print(\"Pattern features added\")\n    return df\n\ndef parse_salary_range(df):\n    MONTH_MAP = {\n        'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,\n        'jul': 7, 'aug': 8, 'sep': 9, 'sept': 9, 'oct': 10, 'nov': 11, 'dec': 12\n    }   \n    \n    def parse_salary(s):\n        if not s or str(s).lower() == 'unknown':\n            return (0, 0)\n\n        s = str(s).strip()\n        \n        if '-' not in s:\n            try: \n                val = int(s)\n                return (val, val)\n            except ValueError: \n                return (0, 0)\n\n        left, right = [v.strip() for v in s.split('-', 1)]\n\n        def val(v):\n            return int(v) if v.isdigit() else MONTH_MAP.get(v.lower())\n\n        l_val, r_val = val(left), val(right)\n        \n        if l_val is not None and r_val is not None:\n            return (l_val, r_val)\n        else:\n            return (0, 0)\n\n    df[['salary_min', 'salary_max']] = df['salary_range'].apply(\n        lambda x: pd.Series(parse_salary(x))\n    )\n    \n    print(\"Salary parsing complete\")\n    return df\n\nfrom sentence_transformers import SentenceTransformer\n\ndef sentenceEmbedding(df, text_cols, model=None, model_name='all-MiniLM-L6-v2', device=None):\n    if model is None:\n        model = SentenceTransformer(model_name, device=device)\n        print(f\"Loaded SentenceTransformer model: {model_name}\")\n\n    for col in text_cols:\n        tqdm.pandas(desc=f\"Embedding {col}\")\n        embeddings = df[col].progress_apply(\n            lambda x: model.encode(str(x)) if pd.notna(x) else np.zeros(model.get_sentence_embedding_dimension())\n        )\n        df[f\"{col}_embedding\"] = embeddings\n\n    return df, model\n\ndef expand_embeddings(df, embedding_cols):\n    \"\"\"\n    Expand embedding arrays (stored as numpy arrays in cells) into separate columns.\n    \"\"\"\n    print(\"\\n--- Expanding Embeddings ---\")\n    \n    for col in embedding_cols:\n        if col not in df.columns:\n            print(f\"Warning: {col} not found in dataframe\")\n            continue\n\n        embedding_matrix = np.vstack(df[col].values)\n        n_dims = embedding_matrix.shape[1]\n        col_names = [f\"{col}_dim_{i}\" for i in range(n_dims)]\n        \n        # df from embeddings\n        embedding_df = pd.DataFrame(embedding_matrix, columns=col_names, index=df.index)\n        df = pd.concat([df, embedding_df], axis=1)\n        df = df.drop(columns=[col])\n        \n        print(f\"Expanded {col}: {n_dims} dimensions\")\n    \n    print(\"Embedding expansion complete\")\n    return df\n\ndef build_tfidf(\n    df, \n    text_cols, \n    word_ngrams=(1, 2), \n    char_ngrams=(3, 5),\n    min_df=3,            # ignore terms appearing in <3 docs\n    max_df=0.8,          # ignore terms appearing in >80% of docs\n    vectorizers=None     # Pre-fitted vectorizers for test data\n):\n    \"\"\"\n    Build TF-IDF features for text columns.\n    Creates both word-level and character-level n-grams.\n    \"\"\"\n    tfidf_results = {}\n    vectorizers_out = {} if vectorizers is None else vectorizers\n    is_training = vectorizers is None\n\n    for col in text_cols:\n        print(f'Processing {col}...')\n        \n        # Ensure all values are strings (should already be from null handling)\n        text_data = df[col].astype(str)\n\n        if is_training:\n            # Training: fit new vectorizers\n            word_vectorizer = TfidfVectorizer(\n                tokenizer=word_tokenize,\n                token_pattern=None,\n                ngram_range=word_ngrams,\n                max_features=2000,\n                min_df=min_df,\n                max_df=max_df\n            )\n            word_vec = word_vectorizer.fit_transform(text_data)\n\n            char_vectorizer = TfidfVectorizer(\n                analyzer='char_wb',\n                ngram_range=char_ngrams,\n                max_features=3000,\n                min_df=min_df,\n                max_df=max_df\n            )\n            char_vec = char_vectorizer.fit_transform(text_data)\n            \n            # Store vectorizers for reuse\n            vectorizers_out[f'{col}_word'] = word_vectorizer\n            vectorizers_out[f'{col}_char'] = char_vectorizer\n        else:\n            # Test: use pre-fitted vectorizers\n            word_vectorizer = vectorizers_out[f'{col}_word']\n            word_vec = word_vectorizer.transform(text_data)\n            \n            char_vectorizer = vectorizers_out[f'{col}_char']\n            char_vec = char_vectorizer.transform(text_data)\n\n        tfidf_results[col] = {\n            \"word_tfidf\": word_vec,\n            \"char_tfidf\": char_vec,\n            \"word_features\": word_vectorizer.get_feature_names_out(),\n            \"char_features\": char_vectorizer.get_feature_names_out()\n        }\n\n    if is_training:\n        print(\"TF-IDF feature extraction complete (fitted new vectorizers)\")\n    else:\n        print(\"TF-IDF feature extraction complete (used pre-fitted vectorizers)\")\n    \n    return tfidf_results, vectorizers_out\n\ndef merge_tfidf_results(tfidf_results):\n    \"\"\"\n    Merge all TF-IDF matrices (word and char) into a single sparse matrix.\n    Returns both the sparse matrix and a DataFrame representation.\n    \"\"\"\n    all_matrices = []\n    all_feature_names = []\n\n    for col, result in tfidf_results.items():\n        word_features = [f\"{col}_word_{f}\" for f in result[\"word_features\"]]\n        all_feature_names.extend(word_features)\n        all_matrices.append(result[\"word_tfidf\"])\n        \n        char_features = [f\"{col}_char_{f}\" for f in result[\"char_features\"]]\n        all_feature_names.extend(char_features)\n        all_matrices.append(result[\"char_tfidf\"])\n\n    # Combine all sparse matrices horizontally\n    combined_matrix = hstack(all_matrices).tocsr()\n\n    # Create a sparse DataFrame (efficient for large feature sets)\n    tfidf_df = pd.DataFrame.sparse.from_spmatrix(combined_matrix, columns=all_feature_names)\n\n    print(f\"TF-IDF merge complete: {combined_matrix.shape[0]} samples × {combined_matrix.shape[1]} features\")\n    return combined_matrix, tfidf_df, all_feature_names\n\ndef apply_svd_reduction(tfidf_matrix, n_components=500, random_state=42):\n    \"\"\"\n    Apply TruncatedSVD to reduce TF-IDF dimensionality.\n    \"\"\"\n    print(f\"Applying TruncatedSVD: {tfidf_matrix.shape[1]} features -> {n_components} components\")\n    \n    svd = TruncatedSVD(n_components=n_components, random_state=random_state)\n    svd_matrix = svd.fit_transform(tfidf_matrix)\n    return svd_matrix, svd\n\ndef parse_location(df):\n    def parse_location_parts(loc):\n        if not isinstance(loc, str) or loc == 'unknown':\n            return (\"unknown\", \"unknown\", \"unknown\")\n        \n        # Split by comma and strip, replacing empty strings with \"unknown\"\n        parts = [p.strip() if p.strip() else \"unknown\" for p in loc.split(',')]\n        while len(parts) < 3:\n            parts.append(\"unknown\")\n        return (parts[0], parts[1], parts[2])\n    \n    df_loc = df['location'].apply(parse_location_parts)\n    df['location_country'] = df_loc.apply(lambda x: x[0])\n    df['location_state'] = df_loc.apply(lambda x: x[1])\n    df['location_city'] = df_loc.apply(lambda x: x[2])\n    \n    # Ensure no NaN values remain (extra safety check)\n    df['location_country'] = df['location_country'].fillna('unknown')\n    df['location_state'] = df['location_state'].fillna('unknown')\n    df['location_city'] = df['location_city'].fillna('unknown')\n    \n    print(\"Location parsing complete\")\n    return df\n\ndef preprocess_df(initial_df, n_svd_components=500, svd_model=None, tfidf_vectorizers=None):\n    df = initial_df.copy()\n    \n    # STEP 3: Add pattern features (on raw text)\n    df = addWordPatterns(df)\n    validate_nulls(df, \"pattern features\")\n    \n    # STEP 4: Normalize text\n    print(\"\\n--- Text Processing ---\")\n    df = apply_text_normalization(df, TEXT_COLS)\n    \n    # STEP 5: Split joined words\n    df = apply_split_df(df, TEXT_COLS)\n    \n    # STEP 6: Generate embeddings (before stopword removal)\n    print(\"\\n--- Generating Embeddings ---\")\n    sentence_cols = ['description', 'requirements', 'benefits', 'company_profile']\n    df = sentenceEmbedding(df, text_cols=sentence_cols)\n    validate_nulls(df, \"sentence embeddings\")\n    \n    # STEP 7: Remove stopwords\n    df = remove_stopwords_df(df, TEXT_COLS)\n    \n    # STEP 8: Lemmatize\n    df = lemmatize_df(df, TEXT_COLS)\n    \n    # STEP 9: Build TF-IDF for all text columns\n    print(\"\\n--- Building TF-IDF Features ---\")\n    tfidf_results, tfidf_vectorizers = build_tfidf(df, TEXT_COLS, vectorizers=tfidf_vectorizers)\n    tfidf_matrix, tfidf_df, tfidf_feature_names = merge_tfidf_results(tfidf_results)\n    \n    # STEP 10: Apply TruncatedSVD to reduce TF-IDF dimensions\n    print(\"\\n--- Applying TruncatedSVD Dimensionality Reduction ---\")\n    if svd_model is None:\n        # Training data: fit new SVD model\n        svd_matrix, svd_model = apply_svd_reduction(tfidf_matrix, n_components=n_svd_components)\n        print(\"✓ Fitted new SVD model\")\n    else:\n        # Test data: use pre-fitted SVD model\n        svd_matrix = svd_model.transform(tfidf_matrix)\n        explained_variance = svd_model.explained_variance_ratio_.sum()\n        print(f\"✓ Used pre-fitted SVD model\")\n        print(f\"Explained variance: {explained_variance:.2%}\")\n        print(f\"SVD output shape: {svd_matrix.shape}\")\n    \n    # Convert SVD matrix to DataFrame\n    svd_col_names = [f\"tfidf_svd_{i}\" for i in range(svd_matrix.shape[1])]\n    svd_df = pd.DataFrame(svd_matrix, columns=svd_col_names, index=df.index)\n    \n    # STEP 11: Parse salary range\n    df = parse_salary_range(df)\n    validate_nulls(df, \"salary parsing\")\n    \n    # STEP 12: Parse location\n    df = parse_location(df)\n    validate_nulls(df, \"location parsing\")\n    \n    # STEP 13: Prepare final features\n    categorical_cols = [\n        'employment_type_clean', \n        'required_experience', \n        'required_education', \n        'industry', \n        'function',\n        'department',\n        'location_country', \n        'location_state', \n        'location_city'\n    ]\n    \n    for col in categorical_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(str)\n    \n    # Drop intermediate columns (processed text, original location/employment/salary)\n    df_features = df.drop(\n        columns=TEXT_COLS + ['dedup_key', 'location', 'employment_type', 'salary_range'], \n        errors='ignore'\n    )\n    \n    # STEP 14: Expand embeddings into separate columns\n    embedding_cols = [f'{col}_embedding' for col in sentence_cols]\n    df_features = expand_embeddings(df_features, embedding_cols)\n    \n    # STEP 15: Add SVD-reduced TF-IDF features\n    print(\"\\n--- Adding TF-IDF SVD Features ---\")\n    df_features = pd.concat([df_features, svd_df], axis=1)\n    print(f\"Added {svd_matrix.shape[1]} TF-IDF SVD features\")\n    \n    validate_nulls(df_features, \"final processing\")\n    \n    print(f\"\\n=== Final Feature Summary ===\")\n    print(f\"Total features: {df_features.shape[1]}\")\n    print(f\"Total samples: {df_features.shape[0]}\")\n    \n    return df_features, svd_model, tfidf_vectorizers"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5df5f89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alden\\Desktop\\Code\\FraudJob\\.venv\\Lib\\site-packages\\hyperopt\\atpe.py:19: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_recall_curve, auc, \n",
    "    classification_report, confusion_matrix,\n",
    "    f1_score, accuracy_score, precision_score, \n",
    "    recall_score, average_precision_score, roc_curve\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06e08aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (14045, 2072)\n",
      "Test shape: (3561, 2072)\n",
      "\n",
      "Target distribution in train:\n",
      "fraudulent\n",
      "0    13383\n",
      "1      662\n",
      "Name: count, dtype: int64\n",
      "Removed 399 duplicate rows\n",
      "\n",
      "Raw data shape: (17481, 20)\n"
     ]
    }
   ],
   "source": [
    "target_col = 'fraudulent'\n",
    "id_col = 'job_id'\n",
    "\n",
    "# load preprocessed data for hyperparameter tuning\n",
    "train_df = pd.read_csv('../data/processed_train_features.csv')\n",
    "test_df = pd.read_csv('../data/processed_test_features.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"\\nTarget distribution in train:\")\n",
    "print(train_df[target_col].value_counts())\n",
    "\n",
    "# load raw data for final k-fold training\n",
    "raw_data = pd.read_csv('../data/fake_job_postings.csv')\n",
    "raw_data = handle_nulls_upfront(raw_data)\n",
    "raw_data = cleanAndDeduplicate(raw_data)\n",
    "raw_data = raw_data.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nRaw data shape: {raw_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d82b6267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hp search space\n",
    "search_space = {\n",
    "    'max_depth': hp.choice('max_depth', range(3, 10)),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.3)),\n",
    "    'n_estimators': hp.choice('n_estimators', range(100, 500, 50)),\n",
    "    'min_child_weight': hp.choice('min_child_weight', range(1, 8)),\n",
    "    'gamma': hp.uniform('gamma', 0, 0.5),\n",
    "    'subsample': hp.uniform('subsample', 0.6, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 1.0),\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', np.log(1e-5), np.log(10)),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', np.log(1e-5), np.log(10))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04hh3v80uf8k",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_preprocessed(params):\n",
    "    \"\"\"\n",
    "    hyperopt objective using preprocessed train/test split\n",
    "    \"\"\"\n",
    "    params_int = {\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'n_estimators': int(params['n_estimators']),\n",
    "        'min_child_weight': int(params['min_child_weight'])\n",
    "    }\n",
    "    params_float = {k: v for k, v in params.items() if k not in params_int}\n",
    "    params_combined = {**params_int, **params_float}\n",
    "    \n",
    "    # prepare features\n",
    "    X_train = train_df.drop(columns=[id_col, target_col], errors='ignore')\n",
    "    y_train = train_df[target_col]\n",
    "    X_test = test_df.drop(columns=[id_col, target_col], errors='ignore')\n",
    "    y_test = test_df[target_col]\n",
    "    \n",
    "    # categorical columns to 'category' dtype\n",
    "    categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    for col in categorical_cols:\n",
    "        X_train[col] = X_train[col].astype('category')\n",
    "        X_test[col] = X_test[col].astype('category')\n",
    "    \n",
    "    # apply smote\n",
    "    smote = SMOTE(random_state=RANDOM_STATE)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    X_train_resampled = pd.DataFrame(X_train_resampled, columns=X_train.columns)\n",
    "    for col in categorical_cols:\n",
    "        X_train_resampled[col] = X_train_resampled[col].astype('category')\n",
    "    \n",
    "    # train model\n",
    "    model = XGBClassifier(\n",
    "        **params_combined,\n",
    "        enable_categorical=True,\n",
    "        tree_method='hist',\n",
    "        random_state=RANDOM_STATE,\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False\n",
    "    )\n",
    "    model.fit(X_train_resampled, y_train_resampled, verbose=False)\n",
    "    \n",
    "    # evaluate on test set\n",
    "    y_test_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "    \n",
    "    return {'loss': -roc_auc, 'status': STATUS_OK, 'roc_auc': roc_auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b9aen97zd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: Input contains NaN\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# run hyperparameter optimization on preprocessed data\u001b[39;00m\n\u001b[32m      2\u001b[39m trials = Trials()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m best_params = \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobjective_preprocessed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtpe\u001b[49m\u001b[43m.\u001b[49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrstate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault_rng\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRANDOM_STATE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest hyperparameters found:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alden\\Desktop\\Code\\FraudJob\\.venv\\Lib\\site-packages\\hyperopt\\fmin.py:540\u001b[39m, in \u001b[36mfmin\u001b[39m\u001b[34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[39m\n\u001b[32m    537\u001b[39m     fn = __objective_fmin_wrapper(fn)\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_trials_fmin \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trials, \u001b[33m\"\u001b[39m\u001b[33mfmin\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrials\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m        \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m=\u001b[49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrstate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m        \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    559\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(trials_save_file):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alden\\Desktop\\Code\\FraudJob\\.venv\\Lib\\site-packages\\hyperopt\\base.py:671\u001b[39m, in \u001b[36mTrials.fmin\u001b[39m\u001b[34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;66;03m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[32m    667\u001b[39m \u001b[38;5;66;03m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[32m    668\u001b[39m \u001b[38;5;66;03m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[32m    669\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfmin\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fmin\n\u001b[32m--> \u001b[39m\u001b[32m671\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m=\u001b[49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrstate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_trials_fmin\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# -- prevent recursion\u001b[39;49;00m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alden\\Desktop\\Code\\FraudJob\\.venv\\Lib\\site-packages\\hyperopt\\fmin.py:586\u001b[39m, in \u001b[36mfmin\u001b[39m\u001b[34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[39m\n\u001b[32m    583\u001b[39m rval.catch_eval_exceptions = catch_eval_exceptions\n\u001b[32m    585\u001b[39m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m586\u001b[39m \u001b[43mrval\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[32m    589\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials.trials) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alden\\Desktop\\Code\\FraudJob\\.venv\\Lib\\site-packages\\hyperopt\\fmin.py:364\u001b[39m, in \u001b[36mFMinIter.exhaust\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    363\u001b[39m     n_done = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.trials)\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    365\u001b[39m     \u001b[38;5;28mself\u001b[39m.trials.refresh()\n\u001b[32m    366\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alden\\Desktop\\Code\\FraudJob\\.venv\\Lib\\site-packages\\hyperopt\\fmin.py:300\u001b[39m, in \u001b[36mFMinIter.run\u001b[39m\u001b[34m(self, N, block_until_done)\u001b[39m\n\u001b[32m    297\u001b[39m     time.sleep(\u001b[38;5;28mself\u001b[39m.poll_interval_secs)\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    299\u001b[39m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28mself\u001b[39m.trials.refresh()\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trials_save_file != \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alden\\Desktop\\Code\\FraudJob\\.venv\\Lib\\site-packages\\hyperopt\\fmin.py:178\u001b[39m, in \u001b[36mFMinIter.serial_evaluate\u001b[39m\u001b[34m(self, N)\u001b[39m\n\u001b[32m    176\u001b[39m ctrl = base.Ctrl(\u001b[38;5;28mself\u001b[39m.trials, current_trial=trial)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdomain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    180\u001b[39m     logger.error(\u001b[33m\"\u001b[39m\u001b[33mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % \u001b[38;5;28mstr\u001b[39m(e))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alden\\Desktop\\Code\\FraudJob\\.venv\\Lib\\site-packages\\hyperopt\\base.py:892\u001b[39m, in \u001b[36mDomain.evaluate\u001b[39m\u001b[34m(self, config, ctrl, attach_attachments)\u001b[39m\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    884\u001b[39m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[32m    885\u001b[39m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[32m    886\u001b[39m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[32m    887\u001b[39m     pyll_rval = pyll.rec_eval(\n\u001b[32m    888\u001b[39m         \u001b[38;5;28mself\u001b[39m.expr,\n\u001b[32m    889\u001b[39m         memo=memo,\n\u001b[32m    890\u001b[39m         print_node_on_error=\u001b[38;5;28mself\u001b[39m.rec_eval_print_node_on_error,\n\u001b[32m    891\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m892\u001b[39m     rval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np.number)):\n\u001b[32m    895\u001b[39m     dict_rval = {\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[33m\"\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m\"\u001b[39m: STATUS_OK}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mobjective_preprocessed\u001b[39m\u001b[34m(params)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# apply smote\u001b[39;00m\n\u001b[32m     26\u001b[39m smote = SMOTE(random_state=RANDOM_STATE)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m X_train_resampled, y_train_resampled = \u001b[43msmote\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m X_train_resampled = pd.DataFrame(X_train_resampled, columns=X_train.columns)\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m categorical_cols:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alden\\Desktop\\Code\\FraudJob\\.venv\\Lib\\site-packages\\imblearn\\base.py:202\u001b[39m, in \u001b[36mBaseSampler.fit_resample\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_resample\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, **params):\n\u001b[32m    182\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[32m    183\u001b[39m \n\u001b[32m    184\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    200\u001b[39m \u001b[33;03m        The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alden\\Desktop\\Code\\FraudJob\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alden\\Desktop\\Code\\FraudJob\\.venv\\Lib\\site-packages\\imblearn\\base.py:99\u001b[39m, in \u001b[36mSamplerMixin.fit_resample\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m     97\u001b[39m check_classification_targets(y)\n\u001b[32m     98\u001b[39m arrays_transformer = ArraysTransformer(X, y)\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m X, y, binarize_y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[38;5;28mself\u001b[39m.sampling_strategy_ = check_sampling_strategy(\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mself\u001b[39m.sampling_strategy, y, \u001b[38;5;28mself\u001b[39m._sampling_type\n\u001b[32m    103\u001b[39m )\n\u001b[32m    105\u001b[39m output = \u001b[38;5;28mself\u001b[39m._fit_resample(X, y, **params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alden\\Desktop\\Code\\FraudJob\\.venv\\Lib\\site-packages\\imblearn\\base.py:157\u001b[39m, in \u001b[36mBaseSampler._check_X_y\u001b[39m\u001b[34m(self, X, y, accept_sparse)\u001b[39m\n\u001b[32m    155\u001b[39m     accept_sparse = [\u001b[33m\"\u001b[39m\u001b[33mcsr\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcsc\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    156\u001b[39m y, binarize_y = check_target_type(y, indicate_one_vs_all=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m X, y, binarize_y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alden\\Desktop\\Code\\FraudJob\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2971\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2969\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2970\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2971\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alden\\Desktop\\Code\\FraudJob\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1368\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1362\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1363\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1364\u001b[39m     )\n\u001b[32m   1366\u001b[39m ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1385\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1387\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alden\\Desktop\\Code\\FraudJob\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1105\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1099\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1101\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1114\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alden\\Desktop\\Code\\FraudJob\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:105\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_array_api \u001b[38;5;129;01mand\u001b[39;00m X.dtype == np.dtype(\u001b[33m\"\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _object_dtype_isnan(X).any():\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInput contains NaN\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# We need only consider float arrays, hence can early return for all else.\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m xp.isdtype(X.dtype, (\u001b[33m\"\u001b[39m\u001b[33mreal floating\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcomplex floating\u001b[39m\u001b[33m\"\u001b[39m)):\n",
      "\u001b[31mValueError\u001b[39m: Input contains NaN"
     ]
    }
   ],
   "source": [
    "# run hyperparameter optimization on preprocessed data\n",
    "trials = Trials()\n",
    "best_params = fmin(\n",
    "    fn=objective_preprocessed,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=50,\n",
    "    trials=trials,\n",
    "    rstate=np.random.default_rng(RANDOM_STATE),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(\"=\"*50)\n",
    "for k, v in best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# get best roc-auc from trials\n",
    "best_trial = trials.best_trial\n",
    "print(f\"\\nBest ROC-AUC: {best_trial['result']['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ptbresklvy",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_final = {\n",
    "    'max_depth': int(best_params['max_depth']),\n",
    "    'learning_rate': float(best_params['learning_rate']),\n",
    "    'n_estimators': int(best_params['n_estimators']),\n",
    "    'min_child_weight': int(best_params['min_child_weight']),\n",
    "    'gamma': float(best_params['gamma']),\n",
    "    'subsample': float(best_params['subsample']),\n",
    "    'colsample_bytree': float(best_params['colsample_bytree']),\n",
    "    'reg_alpha': float(best_params['reg_alpha']),\n",
    "    'reg_lambda': float(best_params['reg_lambda'])\n",
    "}\n",
    "\n",
    "print(\"Final parameters for K-fold training:\")\n",
    "for k, v in best_params_final.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skavb56xsk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train final model with k-fold cv on raw data using best hyperparameters\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training final model with K-fold CV on raw data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X = raw_data.drop(columns=[target_col])\n",
    "y = raw_data[target_col]\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "fold_results = []\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "all_y_pred_proba = []\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"\\nFold {fold_idx}/5\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    X_train_raw = X.iloc[train_idx].reset_index(drop=True)\n",
    "    X_val_raw = X.iloc[val_idx].reset_index(drop=True)\n",
    "    y_train = y.iloc[train_idx].reset_index(drop=True)\n",
    "    y_val = y.iloc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    # preprocess within fold\n",
    "    X_train_processed, svd_model, tfidf_vectorizers = preprocess_df(X_train_raw)\n",
    "    X_val_processed, _, _ = preprocess_df(\n",
    "        X_val_raw, \n",
    "        svd_model=svd_model, \n",
    "        tfidf_vectorizers=tfidf_vectorizers\n",
    "    )\n",
    "    \n",
    "    X_val_processed = X_val_processed.reindex(columns=X_train_processed.columns, fill_value=0)\n",
    "    \n",
    "    categorical_cols = X_train_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "    if target_col in categorical_cols:\n",
    "        categorical_cols.remove(target_col)\n",
    "    if id_col in categorical_cols:\n",
    "        categorical_cols.remove(id_col)\n",
    "    \n",
    "    X_train_features = X_train_processed.drop(columns=[id_col, target_col], errors='ignore')\n",
    "    X_val_features = X_val_processed.drop(columns=[id_col, target_col], errors='ignore')\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in X_train_features.columns:\n",
    "            X_train_features[col] = X_train_features[col].astype('category')\n",
    "        if col in X_val_features.columns:\n",
    "            X_val_features[col] = X_val_features[col].astype('category')\n",
    "    \n",
    "    # apply smote\n",
    "    smote = SMOTE(random_state=RANDOM_STATE)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_features, y_train)\n",
    "    \n",
    "    X_train_resampled = pd.DataFrame(X_train_resampled, columns=X_train_features.columns)\n",
    "    for col in categorical_cols:\n",
    "        if col in X_train_resampled.columns:\n",
    "            X_train_resampled[col] = X_train_resampled[col].astype('category')\n",
    "    \n",
    "    # train with best hyperparameters\n",
    "    model = XGBClassifier(\n",
    "        **best_params_final,\n",
    "        enable_categorical=True,\n",
    "        tree_method='hist',\n",
    "        random_state=RANDOM_STATE,\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False\n",
    "    )\n",
    "    model.fit(X_train_resampled, y_train_resampled, verbose=False)\n",
    "    \n",
    "    # predictions\n",
    "    y_val_pred_proba = model.predict_proba(X_val_features)[:, 1]\n",
    "    y_val_pred = model.predict(X_val_features)\n",
    "    \n",
    "    # metrics\n",
    "    roc_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "    pr_auc = average_precision_score(y_val, y_val_pred_proba)\n",
    "    f1 = f1_score(y_val, y_val_pred)\n",
    "    precision = precision_score(y_val, y_val_pred)\n",
    "    recall = recall_score(y_val, y_val_pred)\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    fold_results.append({\n",
    "        'fold': fold_idx,\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "    \n",
    "    all_y_true.extend(y_val.tolist())\n",
    "    all_y_pred.extend(y_val_pred.tolist())\n",
    "    all_y_pred_proba.extend(y_val_pred_proba.tolist())\n",
    "    \n",
    "    print(f\"ROC-AUC: {roc_auc:.4f} | PR-AUC: {pr_auc:.4f} | F1: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f} | Recall: {recall:.4f} | Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# summary\n",
    "results_df = pd.DataFrame(fold_results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"K-Fold Cross-Validation Summary\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\nMean ± Std:\")\n",
    "for col in ['roc_auc', 'pr_auc', 'f1', 'precision', 'recall', 'accuracy']:\n",
    "    print(f\"  {col:12s}: {results_df[col].mean():.4f} ± {results_df[col].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ndkg2qena0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "cm = confusion_matrix(all_y_true, all_y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['legitimate', 'fraudulent'],\n",
    "            yticklabels=['legitimate', 'fraudulent'])\n",
    "plt.title('confusion matrix (aggregated across folds)')\n",
    "plt.ylabel('true label')\n",
    "plt.xlabel('predicted label')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nclassification report:\")\n",
    "print(classification_report(all_y_true, all_y_pred, \n",
    "                          target_names=['legitimate', 'fraudulent']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ihhld6kzmb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc curve\n",
    "fpr, tpr, _ = roc_curve(all_y_true, all_y_pred_proba)\n",
    "roc_auc_agg = roc_auc_score(all_y_true, all_y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'roc curve (auc = {roc_auc_agg:.4f})', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='random classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('true positive rate')\n",
    "plt.title('roc curve (aggregated across folds)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ptgh7te6qe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision-recall curve\n",
    "precision_vals, recall_vals, _ = precision_recall_curve(all_y_true, all_y_pred_proba)\n",
    "pr_auc_agg = average_precision_score(all_y_true, all_y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall_vals, precision_vals, label=f'pr curve (auc = {pr_auc_agg:.4f})', linewidth=2)\n",
    "plt.xlabel('recall')\n",
    "plt.ylabel('precision')\n",
    "plt.title('precision-recall curve (aggregated across folds)')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraudjob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}