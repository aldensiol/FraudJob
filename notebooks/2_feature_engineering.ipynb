{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267488de",
   "metadata": {},
   "source": [
    "**Feature Engineering Pipeline**\n",
    "\n",
    "**Null Handling Strategy:**\n",
    "- Text columns: Replace nulls with empty string `\"\"`\n",
    "- Categorical columns: Replace nulls with `\"unknown\"`\n",
    "- Numerical columns (salary): Replace nulls with `0` for min/max\n",
    "- Embeddings: Use empty string embeddings instead of zero vectors\n",
    "- Validation: Check for nulls after each major transformation stage\n",
    "\n",
    "**Pipeline Steps:**\n",
    "1. **Null Handling** (Upfront standardization)\n",
    "2. Deduplication\n",
    "3. **Pattern Features** (On raw text - URLs, emails, phone numbers, symbols)\n",
    "4. Standardize & Clean Text\n",
    "5. Split Joined Words (CamelCase and wordninja)\n",
    "6. **Embeddings** (Before stopword removal - sentence-transformers)\n",
    "7. Stopword Removal\n",
    "8. Lemmatization\n",
    "9. **TF-IDF** (Word + Char n-grams - fit on train, transform on test)\n",
    "10. **TruncatedSVD** (Dimensionality reduction: 25K → 500 features, ~XX% variance)\n",
    "11. Parse Salary (min/max)\n",
    "12. Parse Location (country/state/city)\n",
    "13. **Expand Embeddings** (Convert arrays to individual columns)\n",
    "14. Structured Features (categorical, binary)\n",
    "15. Final Validation\n",
    "\n",
    "**Train/Test Split Handling:**\n",
    "- **TF-IDF Vectorizers**: Fitted on training data, reused for test data (no leakage)\n",
    "- **TruncatedSVD Model**: Fitted on training data, reused for test data (no leakage)\n",
    "- Ensures consistent feature space between train and test sets\n",
    "\n",
    "**Output Features:**\n",
    "- Pattern Features: ~20 features (URL/email/phone/symbol counts)\n",
    "- Salary Features: 2 features (min, max)\n",
    "- Binary Features: 3 features (telecommuting, has_logo, has_questions)\n",
    "- Categorical Features: 9 features (employment_type, experience, education, industry, function, department, location x3)\n",
    "- Embedding Features: 1,536 features (384 dims × 4 text columns)\n",
    "- TF-IDF SVD Features: 500 features (reduced from 25,000)\n",
    "- **Total: ~2,070 features** (down from ~26,500 before SVD reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a79848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\alden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\alden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# NLP preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "import wordninja\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "TEXT_COLS = ['title', 'description', 'requirements', 'benefits', 'company_profile']\n",
    "CATEGORICAL_COLS = ['employment_type', 'required_experience', 'required_education', 'industry', 'function', 'location', 'department']\n",
    "BINARY_COLS = ['telecommuting', 'has_company_logo', 'has_questions', 'fraudulent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92a2be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_nulls_upfront(df):\n",
    "    \"\"\"\n",
    "    Handle all null values upfront before any processing.\n",
    "    This ensures consistent null handling throughout the pipeline.\n",
    "    \n",
    "    Strategy:\n",
    "    - Text columns → empty string \"\"\n",
    "    - Categorical columns → \"unknown\"\n",
    "    - salary_range → \"unknown\"\n",
    "    - Binary columns → keep as-is (will be handled during model training)\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Text columns: replace with empty string\n",
    "    for col in TEXT_COLS:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].fillna('').astype(str)\n",
    "            # Also handle explicit null-like strings\n",
    "            df_clean[col] = df_clean[col].replace(['nan', 'NaN', 'None', 'none'], '')\n",
    "    \n",
    "    # Categorical columns: replace with 'unknown'\n",
    "    for col in CATEGORICAL_COLS:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].fillna('unknown').astype(str)\n",
    "            # Handle various null-like values\n",
    "            null_like = ['Not Applicable', 'NaN', 'not applicable', 'Unspecified', \n",
    "                        'Other', 'Others', 'none', 'na', 'n/a', '', ' ', 'nan', 'None']\n",
    "            df_clean[col] = df_clean[col].replace(null_like, 'unknown')\n",
    "    \n",
    "    # Salary range: replace with 'unknown'\n",
    "    if 'salary_range' in df_clean.columns:\n",
    "        df_clean['salary_range'] = df_clean['salary_range'].fillna('unknown').astype(str)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "\n",
    "def validate_nulls(df, stage_name=\"\"):\n",
    "    \"\"\"\n",
    "    Validate that no unexpected nulls exist after processing.\n",
    "    Reports null counts and returns boolean indicating if nulls were found.\n",
    "    \"\"\"\n",
    "    null_counts = df.isnull().sum()\n",
    "    has_nulls = null_counts.any()\n",
    "    \n",
    "    if has_nulls:\n",
    "        print(f\"Nulls found after {stage_name}:\")\n",
    "        print(null_counts[null_counts > 0])\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"No nulls found after {stage_name}\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7xaafaz00vc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanAndDeduplicate(df):\n",
    "    df_cleaning = df.copy()\n",
    "\n",
    "    def simplify_employment_type(x):\n",
    "        if not isinstance(x, str) or x == 'unknown':\n",
    "            return 'unknown'\n",
    "        \n",
    "        x = x.strip().lower()\n",
    "        if x in ['full-time', 'part-time']:\n",
    "            return x\n",
    "        elif x in ['contract', 'temporary']:\n",
    "            return 'non-permanent'\n",
    "        else:\n",
    "            return 'unknown'\n",
    "    \n",
    "    df_cleaning['employment_type_clean'] = df_cleaning['employment_type'].apply(simplify_employment_type)\n",
    "\n",
    "    def comparison_key(row):\n",
    "        emp = None if row['employment_type_clean'] == 'unknown' else row['employment_type_clean']\n",
    "        return (row['location'], row['title'], row['description'], row['requirements'], emp)\n",
    "\n",
    "    df_cleaning['dedup_key'] = df_cleaning.apply(comparison_key, axis=1)\n",
    "    df_deduped = df_cleaning.drop_duplicates(subset=['dedup_key'])\n",
    "    \n",
    "    print(f\"Removed {len(df_cleaning) - len(df_deduped)} duplicate rows\")\n",
    "    return df_deduped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0633e526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_corpus(df, text_cols):\n",
    "    corpus_stats = {}\n",
    "\n",
    "    for col in text_cols:\n",
    "        texts = df[col].fillna(\"\").astype(str).str.lower().tolist()\n",
    "\n",
    "        tokens = []\n",
    "        for t in texts:\n",
    "            tokens.extend([w for w in word_tokenize(t) if len(w) > 2])\n",
    "\n",
    "        corpus_stats[col] = len(set(tokens))\n",
    "\n",
    "    print(corpus_stats)\n",
    "    return corpus_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ed2de51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_text_normalization(df, text_cols):\n",
    "    \"\"\"\n",
    "    Normalize text: lowercase, remove URLs, punctuation, extra whitespace.\n",
    "    \n",
    "    Note: Assumes nulls have been handled - all text should be strings.\n",
    "    Empty strings remain empty and are handled properly.\n",
    "    \"\"\"\n",
    "    def normalize_text(text: str) -> str:\n",
    "        # Handle empty strings (from nulls)\n",
    "        if not text or not text.strip():\n",
    "            return \"\"\n",
    "\n",
    "        text = text.lower().strip()\n",
    "        text = re.sub(r\"http\\S+|www\\S+\", \" \", text)  # remove URLs\n",
    "        text = re.sub(r\"[^a-z\\s']\", \" \", text)  # remove punctuation/numbers except apostrophes\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        return text\n",
    "    \n",
    "    for col in text_cols:\n",
    "        tqdm.pandas(desc=f\"Normalizing {col}\")\n",
    "        df[col] = df[col].progress_apply(normalize_text)\n",
    "    \n",
    "    print(\"Text normalization complete\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_split_df(df, text_cols):\n",
    "    \"\"\"\n",
    "    Split CamelCase and joined words using wordninja.\n",
    "    Examples: 'SmartContract' -> 'Smart Contract', 'makemoney' -> 'make money'\n",
    "    \"\"\"\n",
    "    def split_camel_case(token):\n",
    "        \"\"\"Splits CamelCase tokens: 'SmartContract' -> ['Smart', 'Contract']\"\"\"\n",
    "        return re.sub('([a-z])([A-Z])', r'\\1 \\2', token).split()\n",
    "\n",
    "    def split_joined_words(text, min_len=10):\n",
    "        # Handle empty strings\n",
    "        if not text or not text.strip():\n",
    "            return \"\"\n",
    "        \n",
    "        tokens = text.split()\n",
    "        new_tokens = []\n",
    "\n",
    "        for token in tokens:\n",
    "            # Skip short tokens\n",
    "            if len(token) < min_len:\n",
    "                new_tokens.append(token)\n",
    "                continue\n",
    "\n",
    "            # 1. Try CamelCase split\n",
    "            camel_split = split_camel_case(token)\n",
    "\n",
    "            if len(camel_split) > 1:\n",
    "                # After splitting CamelCase, apply wordninja to each part\n",
    "                final_parts = []\n",
    "                for part in camel_split:\n",
    "                    wn = wordninja.split(part)\n",
    "                    final_parts.extend(wn)\n",
    "                new_tokens.extend(final_parts)\n",
    "                continue\n",
    "\n",
    "            # 2. If no CamelCase, try wordninja directly\n",
    "            wn = wordninja.split(token)\n",
    "            if len(wn) > 1:\n",
    "                new_tokens.extend(wn)\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "\n",
    "        return \" \".join(new_tokens)\n",
    "\n",
    "    for col in text_cols:\n",
    "        tqdm.pandas(desc=f\"Splitting joined words in {col}\")\n",
    "        df[col] = df[col].progress_apply(split_joined_words)\n",
    "    \n",
    "    print(\"Word splitting complete\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5831e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_df(df, text_cols):\n",
    "    \"\"\"\n",
    "    Remove English stopwords from multiple text columns.\n",
    "    Handles empty strings gracefully.\n",
    "    \"\"\"\n",
    "    def remove_stopwords_text(text):\n",
    "        # Handle empty strings\n",
    "        if not text or not text.strip():\n",
    "            return \"\"\n",
    "        \n",
    "        tokens = word_tokenize(text.lower())\n",
    "        clean_tokens = [t for t in tokens if t not in STOPWORDS and len(t) > 2]\n",
    "        return \" \".join(clean_tokens)\n",
    "    \n",
    "    for col in text_cols:\n",
    "        tqdm.pandas(desc=f\"Removing stopwords in {col}\")\n",
    "        df[col] = df[col].progress_apply(remove_stopwords_text)\n",
    "    \n",
    "    print(\"Stopword removal complete\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3abdee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_df(df, text_cols):\n",
    "    \"\"\"\n",
    "    Lemmatize multiple text columns.\n",
    "    Handles empty strings gracefully.\n",
    "    \"\"\"\n",
    "    def lemmatize_text(text):\n",
    "        # Handle empty strings\n",
    "        if not text or not text.strip():\n",
    "            return \"\"\n",
    "        \n",
    "        tokens = word_tokenize(text.lower())\n",
    "        lemmas = [lemmatizer.lemmatize(t) for t in tokens if len(t) > 2]\n",
    "        return \" \".join(lemmas)\n",
    "    \n",
    "    for col in text_cols:\n",
    "        tqdm.pandas(desc=f\"Lemmatizing {col}\")\n",
    "        df[col] = df[col].progress_apply(lemmatize_text)\n",
    "    \n",
    "    print(\"Lemmatization complete\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24e4c07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addWordPatterns(df):\n",
    "    patterns = {\n",
    "        'urls': r'http[s]?://\\S+',\n",
    "        'emails': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
    "        'phone_numbers': r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b',\n",
    "        'money_symbols': r'[$€£¥]',\n",
    "        'other_symbols': r'[©®™]',\n",
    "    }\n",
    "    \n",
    "    def count_patterns(text, patterns):\n",
    "        if not text or not text.strip():\n",
    "            return {k: 0 for k in patterns.keys()}\n",
    "        return {k: len(re.findall(pat, str(text))) for k, pat in patterns.items()}\n",
    "\n",
    "    for col in ['description', 'company_profile', 'requirements', 'benefits']:\n",
    "        if col in df.columns:\n",
    "            feat_df = df[col].apply(lambda x: count_patterns(x, patterns))\n",
    "            feat_df = pd.DataFrame(list(feat_df), index=df.index).add_prefix(f'{col}_')\n",
    "            df = pd.concat([df, feat_df], axis=1)\n",
    "\n",
    "    print(\"Pattern features added\")\n",
    "    return df\n",
    "\n",
    "def parse_salary_range(df):\n",
    "    MONTH_MAP = {\n",
    "        'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,\n",
    "        'jul': 7, 'aug': 8, 'sep': 9, 'sept': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "    }   \n",
    "    \n",
    "    def parse_salary(s):\n",
    "        if not s or str(s).lower() == 'unknown':\n",
    "            return (0, 0)\n",
    "\n",
    "        s = str(s).strip()\n",
    "        \n",
    "        if '-' not in s:\n",
    "            try: \n",
    "                val = int(s)\n",
    "                return (val, val)\n",
    "            except ValueError: \n",
    "                return (0, 0)\n",
    "\n",
    "        left, right = [v.strip() for v in s.split('-', 1)]\n",
    "\n",
    "        def val(v):\n",
    "            return int(v) if v.isdigit() else MONTH_MAP.get(v.lower())\n",
    "\n",
    "        l_val, r_val = val(left), val(right)\n",
    "        \n",
    "        if l_val is not None and r_val is not None:\n",
    "            return (l_val, r_val)\n",
    "        else:\n",
    "            return (0, 0)\n",
    "\n",
    "    df[['salary_min', 'salary_max']] = df['salary_range'].apply(\n",
    "        lambda x: pd.Series(parse_salary(x))\n",
    "    )\n",
    "    \n",
    "    print(\"Salary parsing complete\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b8ac7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alden\\Desktop\\Code\\FraudJob\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def sentenceEmbedding(df, text_cols, model_name='all-MiniLM-L6-v2', device=None):\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "    \n",
    "    for col in text_cols:\n",
    "        tqdm.pandas(desc=f\"Embedding {col}\")\n",
    "        # Generate embeddings as numpy arrays\n",
    "        embeddings = df[col].progress_apply(lambda x: model.encode(str(x)) if pd.notna(x) else np.zeros(model.get_sentence_embedding_dimension()))\n",
    "        df[f\"{col}_embedding\"] = embeddings\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdpop8c06yi",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_embeddings(df, embedding_cols, n_components=50, embedding_svd_models=None):\n",
    "    \"\"\"\n",
    "    Apply dimensionality reduction to embedding arrays using TruncatedSVD.\n",
    "    Reduces 384-dim embeddings to n_components (default: 50) per text field.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Reducing Embedding Dimensions ---\")\n",
    "\n",
    "    is_training = embedding_svd_models is None\n",
    "    embedding_svd_models_out = {} if is_training else embedding_svd_models\n",
    "\n",
    "    for col in embedding_cols:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Warning: {col} not found in dataframe\")\n",
    "            continue\n",
    "\n",
    "        # Stack embeddings into matrix\n",
    "        embedding_matrix = np.vstack(df[col].values)\n",
    "        original_dims = embedding_matrix.shape[1]\n",
    "\n",
    "        if is_training:\n",
    "            # Training: fit new SVD model\n",
    "            svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "            reduced_matrix = svd.fit_transform(embedding_matrix)\n",
    "            embedding_svd_models_out[col] = svd\n",
    "\n",
    "            explained_var = svd.explained_variance_ratio_.sum()\n",
    "            print(f\"{col}: {original_dims} → {n_components} dims (explained variance: {explained_var:.2%})\")\n",
    "        else:\n",
    "            # Test: use pre-fitted SVD model\n",
    "            svd = embedding_svd_models_out[col]\n",
    "            reduced_matrix = svd.transform(embedding_matrix)\n",
    "            print(f\"{col}: {original_dims} → {n_components} dims (using pre-fitted model)\")\n",
    "\n",
    "        # Create DataFrame with reduced dimensions\n",
    "        col_names = [f\"{col}_svd_{i}\" for i in range(n_components)]\n",
    "        reduced_df = pd.DataFrame(reduced_matrix, columns=col_names, index=df.index)\n",
    "\n",
    "        # Add to main DataFrame and drop original\n",
    "        df = pd.concat([df, reduced_df], axis=1)\n",
    "        df = df.drop(columns=[col])\n",
    "\n",
    "    print(\"Embedding reduction complete\")\n",
    "    return df, embedding_svd_models_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f68fcfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tfidf(\n",
    "    df, \n",
    "    text_cols, \n",
    "    word_ngrams=(1, 2), \n",
    "    char_ngrams=(3, 5),\n",
    "    min_df=3,            # ignore terms appearing in <3 docs\n",
    "    max_df=0.8,          # ignore terms appearing in >80% of docs\n",
    "    vectorizers=None     # Pre-fitted vectorizers for test data\n",
    "):\n",
    "    \"\"\"\n",
    "    Build TF-IDF features for text columns.\n",
    "    Creates both word-level and character-level n-grams.\n",
    "    \"\"\"\n",
    "    tfidf_results = {}\n",
    "    vectorizers_out = {} if vectorizers is None else vectorizers\n",
    "    is_training = vectorizers is None\n",
    "\n",
    "    for col in text_cols:\n",
    "        print(f'Processing {col}...')\n",
    "        \n",
    "        # Ensure all values are strings (should already be from null handling)\n",
    "        text_data = df[col].astype(str)\n",
    "\n",
    "        if is_training:\n",
    "            # Training: fit new vectorizers\n",
    "            word_vectorizer = TfidfVectorizer(\n",
    "                tokenizer=word_tokenize,\n",
    "                token_pattern=None,\n",
    "                ngram_range=word_ngrams,\n",
    "                max_features=2000,\n",
    "                min_df=min_df,\n",
    "                max_df=max_df\n",
    "            )\n",
    "            word_vec = word_vectorizer.fit_transform(text_data)\n",
    "\n",
    "            char_vectorizer = TfidfVectorizer(\n",
    "                analyzer='char_wb',\n",
    "                ngram_range=char_ngrams,\n",
    "                max_features=3000,\n",
    "                min_df=min_df,\n",
    "                max_df=max_df\n",
    "            )\n",
    "            char_vec = char_vectorizer.fit_transform(text_data)\n",
    "            \n",
    "            # Store vectorizers for reuse\n",
    "            vectorizers_out[f'{col}_word'] = word_vectorizer\n",
    "            vectorizers_out[f'{col}_char'] = char_vectorizer\n",
    "        else:\n",
    "            # Test: use pre-fitted vectorizers\n",
    "            word_vectorizer = vectorizers_out[f'{col}_word']\n",
    "            word_vec = word_vectorizer.transform(text_data)\n",
    "            \n",
    "            char_vectorizer = vectorizers_out[f'{col}_char']\n",
    "            char_vec = char_vectorizer.transform(text_data)\n",
    "\n",
    "        tfidf_results[col] = {\n",
    "            \"word_tfidf\": word_vec,\n",
    "            \"char_tfidf\": char_vec,\n",
    "            \"word_features\": word_vectorizer.get_feature_names_out(),\n",
    "            \"char_features\": char_vectorizer.get_feature_names_out()\n",
    "        }\n",
    "\n",
    "    if is_training:\n",
    "        print(\"TF-IDF feature extraction complete (fitted new vectorizers)\")\n",
    "    else:\n",
    "        print(\"TF-IDF feature extraction complete (used pre-fitted vectorizers)\")\n",
    "    \n",
    "    return tfidf_results, vectorizers_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03d0d1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tfidf_results(tfidf_results):\n",
    "    \"\"\"\n",
    "    Merge all TF-IDF matrices (word and char) into a single sparse matrix.\n",
    "    Returns both the sparse matrix and a DataFrame representation.\n",
    "    \"\"\"\n",
    "    all_matrices = []\n",
    "    all_feature_names = []\n",
    "\n",
    "    for col, result in tfidf_results.items():\n",
    "        word_features = [f\"{col}_word_{f}\" for f in result[\"word_features\"]]\n",
    "        all_feature_names.extend(word_features)\n",
    "        all_matrices.append(result[\"word_tfidf\"])\n",
    "        \n",
    "        char_features = [f\"{col}_char_{f}\" for f in result[\"char_features\"]]\n",
    "        all_feature_names.extend(char_features)\n",
    "        all_matrices.append(result[\"char_tfidf\"])\n",
    "\n",
    "    # Combine all sparse matrices horizontally\n",
    "    combined_matrix = hstack(all_matrices).tocsr()\n",
    "\n",
    "    # Create a sparse DataFrame (efficient for large feature sets)\n",
    "    tfidf_df = pd.DataFrame.sparse.from_spmatrix(combined_matrix, columns=all_feature_names)\n",
    "\n",
    "    print(f\"TF-IDF merge complete: {combined_matrix.shape[0]} samples × {combined_matrix.shape[1]} features\")\n",
    "    return combined_matrix, tfidf_df, all_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dnachmzkxcu",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_svd_reduction(tfidf_matrix, n_components=500, random_state=42):\n",
    "    \"\"\"\n",
    "    Apply TruncatedSVD to reduce TF-IDF dimensionality.\n",
    "    \"\"\"\n",
    "    print(f\"Applying TruncatedSVD: {tfidf_matrix.shape[1]} features -> {n_components} components\")\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=random_state)\n",
    "    svd_matrix = svd.fit_transform(tfidf_matrix)\n",
    "    return svd_matrix, svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68y1oeels9m",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_location(df):\n",
    "    def parse_location_parts(loc):\n",
    "        if not isinstance(loc, str) or loc == 'unknown':\n",
    "            return (\"unknown\", \"unknown\", \"unknown\")\n",
    "        \n",
    "        # Split by comma and strip, replacing empty strings with \"unknown\"\n",
    "        parts = [p.strip() if p.strip() else \"unknown\" for p in loc.split(',')]\n",
    "        while len(parts) < 3:\n",
    "            parts.append(\"unknown\")\n",
    "        return (parts[0], parts[1], parts[2])\n",
    "    \n",
    "    df_loc = df['location'].apply(parse_location_parts)\n",
    "    df['location_country'] = df_loc.apply(lambda x: x[0])\n",
    "    df['location_state'] = df_loc.apply(lambda x: x[1])\n",
    "    df['location_city'] = df_loc.apply(lambda x: x[2])\n",
    "    \n",
    "    # Ensure no NaN values remain (extra safety check)\n",
    "    df['location_country'] = df['location_country'].fillna('unknown')\n",
    "    df['location_state'] = df['location_state'].fillna('unknown')\n",
    "    df['location_city'] = df['location_city'].fillna('unknown')\n",
    "    \n",
    "    print(\"Location parsing complete\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c497ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(initial_df, n_svd_components=500, n_embedding_components=50, \n",
    "                  svd_model=None, tfidf_vectorizers=None, embedding_svd_models=None):\n",
    "    df = initial_df.copy()\n",
    "    \n",
    "    # STEP 3: Add pattern features (on raw text)\n",
    "    df = addWordPatterns(df)\n",
    "    validate_nulls(df, \"pattern features\")\n",
    "    \n",
    "    # STEP 4: Normalize text\n",
    "    print(\"\\n--- Text Processing ---\")\n",
    "    df = apply_text_normalization(df, TEXT_COLS)\n",
    "    \n",
    "    # STEP 5: Split joined words\n",
    "    df = apply_split_df(df, TEXT_COLS)\n",
    "    \n",
    "    # STEP 6: Generate embeddings (before stopword removal)\n",
    "    print(\"\\n--- Generating Embeddings ---\")\n",
    "    sentence_cols = ['description', 'requirements', 'benefits', 'company_profile']\n",
    "    df = sentenceEmbedding(df, text_cols=sentence_cols)\n",
    "    validate_nulls(df, \"sentence embeddings\")\n",
    "    \n",
    "    # STEP 7: Remove stopwords\n",
    "    df = remove_stopwords_df(df, TEXT_COLS)\n",
    "    \n",
    "    # STEP 8: Lemmatize\n",
    "    df = lemmatize_df(df, TEXT_COLS)\n",
    "    \n",
    "    # STEP 9: Build TF-IDF for all text columns\n",
    "    print(\"\\n--- Building TF-IDF Features ---\")\n",
    "    tfidf_results, tfidf_vectorizers = build_tfidf(df, TEXT_COLS, vectorizers=tfidf_vectorizers)\n",
    "    tfidf_matrix, tfidf_df, tfidf_feature_names = merge_tfidf_results(tfidf_results)\n",
    "    \n",
    "    # STEP 10: Apply TruncatedSVD to reduce TF-IDF dimensions\n",
    "    print(\"\\n--- Applying TruncatedSVD Dimensionality Reduction ---\")\n",
    "    if svd_model is None:\n",
    "        # Training data: fit new SVD model\n",
    "        svd_matrix, svd_model = apply_svd_reduction(tfidf_matrix, n_components=n_svd_components)\n",
    "        print(\"✓ Fitted new SVD model\")\n",
    "    else:\n",
    "        # Test data: use pre-fitted SVD model\n",
    "        svd_matrix = svd_model.transform(tfidf_matrix)\n",
    "        explained_variance = svd_model.explained_variance_ratio_.sum()\n",
    "        print(f\"✓ Used pre-fitted SVD model\")\n",
    "        print(f\"Explained variance: {explained_variance:.2%}\")\n",
    "        print(f\"SVD output shape: {svd_matrix.shape}\")\n",
    "    \n",
    "    # Convert SVD matrix to DataFrame\n",
    "    svd_col_names = [f\"tfidf_svd_{i}\" for i in range(svd_matrix.shape[1])]\n",
    "    svd_df = pd.DataFrame(svd_matrix, columns=svd_col_names, index=df.index)\n",
    "    \n",
    "    # STEP 11: Parse salary range\n",
    "    df = parse_salary_range(df)\n",
    "    validate_nulls(df, \"salary parsing\")\n",
    "    \n",
    "    # STEP 12: Parse location\n",
    "    df = parse_location(df)\n",
    "    validate_nulls(df, \"location parsing\")\n",
    "    \n",
    "    # STEP 13: Prepare final features\n",
    "    categorical_cols = [\n",
    "        'employment_type_clean', \n",
    "        'required_experience', \n",
    "        'required_education', \n",
    "        'industry', \n",
    "        'function',\n",
    "        'department',\n",
    "        'location_country', \n",
    "        'location_state', \n",
    "        'location_city'\n",
    "    ]\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "    \n",
    "    # Drop intermediate columns (processed text, original location/employment/salary)\n",
    "    df_features = df.drop(\n",
    "        columns=TEXT_COLS + ['dedup_key', 'location', 'employment_type', 'salary_range'], \n",
    "        errors='ignore'\n",
    "    )\n",
    "    \n",
    "    # STEP 14: Reduce embeddings using SVD\n",
    "    embedding_cols = [f'{col}_embedding' for col in sentence_cols]\n",
    "    df_features, embedding_svd_models = reduce_embeddings(\n",
    "        df_features, \n",
    "        embedding_cols, \n",
    "        n_components=n_embedding_components,\n",
    "        embedding_svd_models=embedding_svd_models\n",
    "    )\n",
    "    \n",
    "    # STEP 15: Add SVD-reduced TF-IDF features\n",
    "    print(\"\\n--- Adding TF-IDF SVD Features ---\")\n",
    "    df_features = pd.concat([df_features, svd_df], axis=1)\n",
    "    print(f\"Added {svd_matrix.shape[1]} TF-IDF SVD features\")\n",
    "    \n",
    "    validate_nulls(df_features, \"final processing\")\n",
    "    \n",
    "    print(f\"\\n=== Final Feature Summary ===\")\n",
    "    print(f\"Total features: {df_features.shape[1]}\")\n",
    "    print(f\"Total samples: {df_features.shape[0]}\")\n",
    "    \n",
    "    return df_features, svd_model, tfidf_vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8c872ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 399 duplicate rows\n",
      "Pattern features added\n",
      "No nulls found after pattern features\n",
      "\n",
      "--- Text Processing ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing title: 100%|██████████| 13984/13984 [00:00<00:00, 452636.94it/s]\n",
      "Normalizing description: 100%|██████████| 13984/13984 [00:00<00:00, 26736.37it/s]\n",
      "Normalizing requirements: 100%|██████████| 13984/13984 [00:00<00:00, 55113.48it/s]\n",
      "Normalizing benefits: 100%|██████████| 13984/13984 [00:00<00:00, 137905.84it/s]\n",
      "Normalizing company_profile: 100%|██████████| 13984/13984 [00:00<00:00, 51201.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text normalization complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting joined words in title: 100%|██████████| 13984/13984 [00:00<00:00, 94461.68it/s]\n",
      "Splitting joined words in description: 100%|██████████| 13984/13984 [00:06<00:00, 2146.90it/s]\n",
      "Splitting joined words in requirements: 100%|██████████| 13984/13984 [00:04<00:00, 3070.25it/s]\n",
      "Splitting joined words in benefits: 100%|██████████| 13984/13984 [00:01<00:00, 12645.31it/s]\n",
      "Splitting joined words in company_profile: 100%|██████████| 13984/13984 [00:02<00:00, 5396.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word splitting complete\n",
      "\n",
      "--- Generating Embeddings ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding description: 100%|██████████| 13984/13984 [03:05<00:00, 75.51it/s]\n",
      "Embedding requirements: 100%|██████████| 13984/13984 [02:02<00:00, 114.62it/s]\n",
      "Embedding benefits: 100%|██████████| 13984/13984 [01:15<00:00, 184.11it/s]\n",
      "Embedding company_profile: 100%|██████████| 13984/13984 [02:15<00:00, 103.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No nulls found after sentence embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing stopwords in title: 100%|██████████| 13984/13984 [00:00<00:00, 54911.79it/s]\n",
      "Removing stopwords in description: 100%|██████████| 13984/13984 [00:03<00:00, 4070.58it/s]\n",
      "Removing stopwords in requirements: 100%|██████████| 13984/13984 [00:01<00:00, 8866.58it/s]\n",
      "Removing stopwords in benefits: 100%|██████████| 13984/13984 [00:00<00:00, 23742.54it/s]\n",
      "Removing stopwords in company_profile: 100%|██████████| 13984/13984 [00:01<00:00, 8380.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopword removal complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lemmatizing title: 100%|██████████| 13984/13984 [00:01<00:00, 8490.06it/s] \n",
      "Lemmatizing description: 100%|██████████| 13984/13984 [00:04<00:00, 3074.71it/s]\n",
      "Lemmatizing requirements: 100%|██████████| 13984/13984 [00:02<00:00, 5967.54it/s]\n",
      "Lemmatizing benefits: 100%|██████████| 13984/13984 [00:00<00:00, 16229.04it/s]\n",
      "Lemmatizing company_profile: 100%|██████████| 13984/13984 [00:02<00:00, 5784.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization complete\n",
      "\n",
      "--- Building TF-IDF Features ---\n",
      "Processing title...\n",
      "Processing description...\n",
      "Processing requirements...\n",
      "Processing benefits...\n",
      "Processing company_profile...\n",
      "TF-IDF feature extraction complete (fitted new vectorizers)\n",
      "TF-IDF merge complete: 13984 samples × 25000 features\n",
      "\n",
      "--- Applying TruncatedSVD Dimensionality Reduction ---\n",
      "Applying TruncatedSVD: 25000 features -> 500 components\n",
      "✓ Fitted new SVD model\n",
      "Salary parsing complete\n",
      "No nulls found after salary parsing\n",
      "Location parsing complete\n",
      "No nulls found after location parsing\n",
      "\n",
      "--- Expanding Embeddings ---\n",
      "Expanded description_embedding: 384 dimensions\n",
      "Expanded requirements_embedding: 384 dimensions\n",
      "Expanded benefits_embedding: 384 dimensions\n",
      "Expanded company_profile_embedding: 384 dimensions\n",
      "Embedding expansion complete\n",
      "\n",
      "--- Adding TF-IDF SVD Features ---\n",
      "Added 500 TF-IDF SVD features\n",
      "No nulls found after final processing\n",
      "\n",
      "=== Final Feature Summary ===\n",
      "Total features: 2072\n",
      "Total samples: 13984\n",
      "Pattern features added\n",
      "No nulls found after pattern features\n",
      "\n",
      "--- Text Processing ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing title: 100%|██████████| 3497/3497 [00:00<00:00, 463134.86it/s]\n",
      "Normalizing description: 100%|██████████| 3497/3497 [00:00<00:00, 24378.48it/s]\n",
      "Normalizing requirements: 100%|██████████| 3497/3497 [00:00<00:00, 51312.53it/s]\n",
      "Normalizing benefits: 100%|██████████| 3497/3497 [00:00<00:00, 116610.33it/s]\n",
      "Normalizing company_profile: 100%|██████████| 3497/3497 [00:00<00:00, 45208.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text normalization complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting joined words in title: 100%|██████████| 3497/3497 [00:00<00:00, 73217.33it/s]\n",
      "Splitting joined words in description: 100%|██████████| 3497/3497 [00:02<00:00, 1643.91it/s]\n",
      "Splitting joined words in requirements: 100%|██████████| 3497/3497 [00:01<00:00, 2560.86it/s]\n",
      "Splitting joined words in benefits: 100%|██████████| 3497/3497 [00:00<00:00, 10663.86it/s]\n",
      "Splitting joined words in company_profile: 100%|██████████| 3497/3497 [00:00<00:00, 4754.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word splitting complete\n",
      "\n",
      "--- Generating Embeddings ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding description: 100%|██████████| 3497/3497 [00:49<00:00, 70.50it/s]\n",
      "Embedding requirements: 100%|██████████| 3497/3497 [00:31<00:00, 110.10it/s]\n",
      "Embedding benefits: 100%|██████████| 3497/3497 [00:19<00:00, 175.11it/s]\n",
      "Embedding company_profile: 100%|██████████| 3497/3497 [00:34<00:00, 100.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No nulls found after sentence embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing stopwords in title: 100%|██████████| 3497/3497 [00:00<00:00, 67249.32it/s]\n",
      "Removing stopwords in description: 100%|██████████| 3497/3497 [00:00<00:00, 3986.18it/s]\n",
      "Removing stopwords in requirements: 100%|██████████| 3497/3497 [00:00<00:00, 8263.46it/s]\n",
      "Removing stopwords in benefits: 100%|██████████| 3497/3497 [00:00<00:00, 21365.32it/s]\n",
      "Removing stopwords in company_profile: 100%|██████████| 3497/3497 [00:00<00:00, 7682.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopword removal complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lemmatizing title: 100%|██████████| 3497/3497 [00:00<00:00, 51337.67it/s]\n",
      "Lemmatizing description: 100%|██████████| 3497/3497 [00:01<00:00, 2798.73it/s]\n",
      "Lemmatizing requirements: 100%|██████████| 3497/3497 [00:00<00:00, 5240.23it/s]\n",
      "Lemmatizing benefits: 100%|██████████| 3497/3497 [00:00<00:00, 14691.66it/s]\n",
      "Lemmatizing company_profile: 100%|██████████| 3497/3497 [00:00<00:00, 5456.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization complete\n",
      "\n",
      "--- Building TF-IDF Features ---\n",
      "Processing title...\n",
      "Processing description...\n",
      "Processing requirements...\n",
      "Processing benefits...\n",
      "Processing company_profile...\n",
      "TF-IDF feature extraction complete (used pre-fitted vectorizers)\n",
      "TF-IDF merge complete: 3497 samples × 25000 features\n",
      "\n",
      "--- Applying TruncatedSVD Dimensionality Reduction ---\n",
      "✓ Used pre-fitted SVD model\n",
      "Explained variance: 62.39%\n",
      "SVD output shape: (3497, 500)\n",
      "Salary parsing complete\n",
      "No nulls found after salary parsing\n",
      "Location parsing complete\n",
      "No nulls found after location parsing\n",
      "\n",
      "--- Expanding Embeddings ---\n",
      "Expanded description_embedding: 384 dimensions\n",
      "Expanded requirements_embedding: 384 dimensions\n",
      "Expanded benefits_embedding: 384 dimensions\n",
      "Expanded company_profile_embedding: 384 dimensions\n",
      "Embedding expansion complete\n",
      "\n",
      "--- Adding TF-IDF SVD Features ---\n",
      "Added 500 TF-IDF SVD features\n",
      "No nulls found after final processing\n",
      "\n",
      "=== Final Feature Summary ===\n",
      "Total features: 2072\n",
      "Total samples: 3497\n",
      "\n",
      "================================================================================\n",
      "FINAL SHAPES\n",
      "================================================================================\n",
      "Training set: (13984, 2072)\n",
      "Test set: (3497, 2072)\n"
     ]
    }
   ],
   "source": [
    "## For downstream notebooks, we will create train and test datasets\n",
    "\n",
    "df_raw = pd.read_csv('../data/fake_job_postings.csv')\n",
    "df_raw = handle_nulls_upfront(df_raw)\n",
    "df_raw = cleanAndDeduplicate(df_raw)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_raw, test_raw = train_test_split(df_raw, test_size=0.2, random_state=42)\n",
    "\n",
    "# training data \n",
    "df_train, svd_model, tfidf_vectorizers = preprocess_df(train_raw, n_svd_components=500)\n",
    "\n",
    "# test data (reuses the fitted TF-IDF vectorizers and SVD model from training)\n",
    "df_test, _, _ = preprocess_df(test_raw, svd_model=svd_model, tfidf_vectorizers=tfidf_vectorizers)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL SHAPES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Training set: {df_train.shape}\")\n",
    "print(f\"Test set: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9a81831",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('../data/processed_train_features.csv', index=False)\n",
    "df_test.to_csv('../data/processed_test_features.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraudjob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
