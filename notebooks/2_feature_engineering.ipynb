{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267488de",
   "metadata": {},
   "source": [
    "**Feature Engineering Pipeline**\n",
    "\n",
    "**Null Handling Strategy:**\n",
    "- Text columns: Replace nulls with empty string `\"\"`\n",
    "- Categorical columns: Replace nulls with `\"unknown\"`\n",
    "- Numerical columns (salary): Replace nulls with `0` for min/max\n",
    "- Embeddings: Use empty string embeddings instead of zero vectors\n",
    "- Validation: Check for nulls after each major transformation stage\n",
    "\n",
    "**Pipeline Steps:**\n",
    "1. **Null Handling** (Upfront standardization)\n",
    "2. Deduplication\n",
    "3. **Pattern Features** (On raw text - URLs, emails, phone numbers, symbols)\n",
    "4. Standardize & Clean Text\n",
    "5. Split Joined Words (CamelCase and wordninja)\n",
    "6. **Embeddings** (Before stopword removal - sentence-transformers)\n",
    "7. Stopword Removal\n",
    "8. Lemmatization\n",
    "9. **TF-IDF** (Word + Char n-grams - fit on train, transform on test)\n",
    "10. **TruncatedSVD** (Dimensionality reduction: 25K → 500 features, ~XX% variance)\n",
    "11. Parse Salary (min/max)\n",
    "12. Parse Location (country/state/city)\n",
    "13. **Expand Embeddings** (Convert arrays to individual columns)\n",
    "14. Structured Features (categorical, binary)\n",
    "15. Final Validation\n",
    "\n",
    "**Train/Test Split Handling:**\n",
    "- **TF-IDF Vectorizers**: Fitted on training data, reused for test data (no leakage)\n",
    "- **TruncatedSVD Model**: Fitted on training data, reused for test data (no leakage)\n",
    "- Ensures consistent feature space between train and test sets\n",
    "\n",
    "**Output Features:**\n",
    "- Pattern Features: ~20 features (URL/email/phone/symbol counts)\n",
    "- Salary Features: 2 features (min, max)\n",
    "- Binary Features: 3 features (telecommuting, has_logo, has_questions)\n",
    "- Categorical Features: 9 features (employment_type, experience, education, industry, function, department, location x3)\n",
    "- Embedding Features: 1,536 features (384 dims × 4 text columns)\n",
    "- TF-IDF SVD Features: 500 features (reduced from 25,000)\n",
    "- **Total: ~2,070 features** (down from ~26,500 before SVD reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a79848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alden\\Desktop\\Code\\FraudJob\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\alden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\alden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import hstack\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# NLP preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "import wordninja\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "TEXT_COLS = ['title', 'description', 'requirements', 'benefits', 'company_profile']\n",
    "CATEGORICAL_COLS = ['employment_type', 'required_experience', 'required_education', 'industry', 'function', 'location', 'department']\n",
    "BINARY_COLS = ['telecommuting', 'has_company_logo', 'has_questions', 'fraudulent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92a2be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_nulls_upfront(df):\n",
    "    \"\"\"\n",
    "    Handle all null values upfront before any processing.\n",
    "    This ensures consistent null handling throughout the pipeline.\n",
    "    \n",
    "    Strategy:\n",
    "    - Text columns → empty string \"\"\n",
    "    - Categorical columns → \"unknown\"\n",
    "    - salary_range → \"unknown\"\n",
    "    - Binary columns → keep as-is (will be handled during model training)\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Text columns: replace with empty string\n",
    "    for col in TEXT_COLS:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].fillna('').astype(str)\n",
    "            # Also handle explicit null-like strings\n",
    "            df_clean[col] = df_clean[col].replace(['nan', 'NaN', 'None', 'none'], '')\n",
    "    \n",
    "    # Categorical columns: replace with 'unknown'\n",
    "    for col in CATEGORICAL_COLS:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].fillna('unknown').astype(str)\n",
    "            # Handle various null-like values\n",
    "            null_like = ['Not Applicable', 'NaN', 'not applicable', 'Unspecified', \n",
    "                        'Other', 'Others', 'none', 'na', 'n/a', '', ' ', 'nan', 'None']\n",
    "            df_clean[col] = df_clean[col].replace(null_like, 'unknown')\n",
    "    \n",
    "    # Salary range: replace with 'unknown'\n",
    "    if 'salary_range' in df_clean.columns:\n",
    "        df_clean['salary_range'] = df_clean['salary_range'].fillna('unknown').astype(str)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "\n",
    "def validate_nulls(df, stage_name=\"\"):\n",
    "    \"\"\"\n",
    "    Validate that no unexpected nulls exist after processing.\n",
    "    Reports null counts and returns boolean indicating if nulls were found.\n",
    "    \"\"\"\n",
    "    null_counts = df.isnull().sum()\n",
    "    has_nulls = null_counts.any()\n",
    "    \n",
    "    if has_nulls:\n",
    "        print(f\"Nulls found after {stage_name}:\")\n",
    "        print(null_counts[null_counts > 0])\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"No nulls found after {stage_name}\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7xaafaz00vc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanAndDeduplicate(df):\n",
    "    df_cleaning = df.copy()\n",
    "\n",
    "    def simplify_employment_type(x):\n",
    "        if not isinstance(x, str) or x == 'unknown':\n",
    "            return 'unknown'\n",
    "        \n",
    "        x = x.strip().lower()\n",
    "        if x in ['full-time', 'part-time']:\n",
    "            return x\n",
    "        elif x in ['contract', 'temporary']:\n",
    "            return 'non-permanent'\n",
    "        else:\n",
    "            return 'unknown'\n",
    "    \n",
    "    df_cleaning['employment_type_clean'] = df_cleaning['employment_type'].apply(simplify_employment_type)\n",
    "\n",
    "    def comparison_key(row):\n",
    "        emp = None if row['employment_type_clean'] == 'unknown' else row['employment_type_clean']\n",
    "        return (row['location'], row['title'], row['description'], row['requirements'], emp)\n",
    "\n",
    "    df_cleaning['dedup_key'] = df_cleaning.apply(comparison_key, axis=1)\n",
    "    df_deduped = df_cleaning.drop_duplicates(subset=['dedup_key'])\n",
    "    \n",
    "    print(f\"Removed {len(df_cleaning) - len(df_deduped)} duplicate rows\")\n",
    "    return df_deduped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0633e526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_corpus(df, text_cols):\n",
    "    corpus_stats = {}\n",
    "\n",
    "    for col in text_cols:\n",
    "        texts = df[col].fillna(\"\").astype(str).str.lower().tolist()\n",
    "\n",
    "        tokens = []\n",
    "        for t in texts:\n",
    "            tokens.extend([w for w in word_tokenize(t) if len(w) > 2])\n",
    "\n",
    "        corpus_stats[col] = len(set(tokens))\n",
    "\n",
    "    print(corpus_stats)\n",
    "    return corpus_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ed2de51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_text_normalization(df, text_cols):\n",
    "    \"\"\"\n",
    "    Normalize text: lowercase, remove URLs, punctuation, extra whitespace.\n",
    "    \n",
    "    Note: Assumes nulls have been handled - all text should be strings.\n",
    "    Empty strings remain empty and are handled properly.\n",
    "    \"\"\"\n",
    "    def normalize_text(text: str) -> str:\n",
    "        # Handle empty strings (from nulls)\n",
    "        if not text or not text.strip():\n",
    "            return \"\"\n",
    "\n",
    "        text = text.lower().strip()\n",
    "        text = re.sub(r\"http\\S+|www\\S+\", \" \", text)  # remove URLs\n",
    "        text = re.sub(r\"[^a-z\\s']\", \" \", text)  # remove punctuation/numbers except apostrophes\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        return text\n",
    "    \n",
    "    for col in text_cols:\n",
    "        tqdm.pandas(desc=f\"Normalizing {col}\")\n",
    "        df[col] = df[col].progress_apply(normalize_text)\n",
    "    \n",
    "    print(\"Text normalization complete\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_split_df(df, text_cols):\n",
    "    \"\"\"\n",
    "    Split CamelCase and joined words using wordninja.\n",
    "    Examples: 'SmartContract' -> 'Smart Contract', 'makemoney' -> 'make money'\n",
    "    \"\"\"\n",
    "    def split_camel_case(token):\n",
    "        \"\"\"Splits CamelCase tokens: 'SmartContract' -> ['Smart', 'Contract']\"\"\"\n",
    "        return re.sub('([a-z])([A-Z])', r'\\1 \\2', token).split()\n",
    "\n",
    "    def split_joined_words(text, min_len=10):\n",
    "        # Handle empty strings\n",
    "        if not text or not text.strip():\n",
    "            return \"\"\n",
    "        \n",
    "        tokens = text.split()\n",
    "        new_tokens = []\n",
    "\n",
    "        for token in tokens:\n",
    "            # Skip short tokens\n",
    "            if len(token) < min_len:\n",
    "                new_tokens.append(token)\n",
    "                continue\n",
    "\n",
    "            # 1. Try CamelCase split\n",
    "            camel_split = split_camel_case(token)\n",
    "\n",
    "            if len(camel_split) > 1:\n",
    "                # After splitting CamelCase, apply wordninja to each part\n",
    "                final_parts = []\n",
    "                for part in camel_split:\n",
    "                    wn = wordninja.split(part)\n",
    "                    final_parts.extend(wn)\n",
    "                new_tokens.extend(final_parts)\n",
    "                continue\n",
    "\n",
    "            # 2. If no CamelCase, try wordninja directly\n",
    "            wn = wordninja.split(token)\n",
    "            if len(wn) > 1:\n",
    "                new_tokens.extend(wn)\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "\n",
    "        return \" \".join(new_tokens)\n",
    "\n",
    "    for col in text_cols:\n",
    "        tqdm.pandas(desc=f\"Splitting joined words in {col}\")\n",
    "        df[col] = df[col].progress_apply(split_joined_words)\n",
    "    \n",
    "    print(\"Word splitting complete\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5831e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_df(df, text_cols):\n",
    "    \"\"\"\n",
    "    Remove English stopwords from multiple text columns.\n",
    "    Handles empty strings gracefully.\n",
    "    \"\"\"\n",
    "    def remove_stopwords_text(text):\n",
    "        # Handle empty strings\n",
    "        if not text or not text.strip():\n",
    "            return \"\"\n",
    "        \n",
    "        tokens = word_tokenize(text.lower())\n",
    "        clean_tokens = [t for t in tokens if t not in STOPWORDS and len(t) > 2]\n",
    "        return \" \".join(clean_tokens)\n",
    "    \n",
    "    for col in text_cols:\n",
    "        tqdm.pandas(desc=f\"Removing stopwords in {col}\")\n",
    "        df[col] = df[col].progress_apply(remove_stopwords_text)\n",
    "    \n",
    "    print(\"Stopword removal complete\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3abdee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_df(df, text_cols):\n",
    "    \"\"\"\n",
    "    Lemmatize multiple text columns.\n",
    "    Handles empty strings gracefully.\n",
    "    \"\"\"\n",
    "    def lemmatize_text(text):\n",
    "        # Handle empty strings\n",
    "        if not text or not text.strip():\n",
    "            return \"\"\n",
    "        \n",
    "        tokens = word_tokenize(text.lower())\n",
    "        lemmas = [lemmatizer.lemmatize(t) for t in tokens if len(t) > 2]\n",
    "        return \" \".join(lemmas)\n",
    "    \n",
    "    for col in text_cols:\n",
    "        tqdm.pandas(desc=f\"Lemmatizing {col}\")\n",
    "        df[col] = df[col].progress_apply(lemmatize_text)\n",
    "    \n",
    "    print(\"Lemmatization complete\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24e4c07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addWordPatterns(df):\n",
    "    patterns = {\n",
    "        'urls': r'http[s]?://\\S+',\n",
    "        'emails': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
    "        'phone_numbers': r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b',\n",
    "        'money_symbols': r'[$€£¥]',\n",
    "        'other_symbols': r'[©®™]',\n",
    "    }\n",
    "    \n",
    "    def count_patterns(text, patterns):\n",
    "        if not text or not text.strip():\n",
    "            return {k: 0 for k in patterns.keys()}\n",
    "        return {k: len(re.findall(pat, str(text))) for k, pat in patterns.items()}\n",
    "\n",
    "    for col in ['description', 'company_profile', 'requirements', 'benefits']:\n",
    "        if col in df.columns:\n",
    "            feat_df = df[col].apply(lambda x: count_patterns(x, patterns))\n",
    "            feat_df = pd.DataFrame(list(feat_df), index=df.index).add_prefix(f'{col}_')\n",
    "            df = pd.concat([df, feat_df], axis=1)\n",
    "\n",
    "    print(\"Pattern features added\")\n",
    "    return df\n",
    "\n",
    "def parse_salary_range(df):\n",
    "    MONTH_MAP = {\n",
    "        'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,\n",
    "        'jul': 7, 'aug': 8, 'sep': 9, 'sept': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "    }   \n",
    "    \n",
    "    def parse_salary(s):\n",
    "        if not s or str(s).lower() == 'unknown':\n",
    "            return (0, 0)\n",
    "\n",
    "        s = str(s).strip()\n",
    "        \n",
    "        if '-' not in s:\n",
    "            try: \n",
    "                val = int(s)\n",
    "                return (val, val)\n",
    "            except ValueError: \n",
    "                return (0, 0)\n",
    "\n",
    "        left, right = [v.strip() for v in s.split('-', 1)]\n",
    "\n",
    "        def val(v):\n",
    "            return int(v) if v.isdigit() else MONTH_MAP.get(v.lower())\n",
    "\n",
    "        l_val, r_val = val(left), val(right)\n",
    "        \n",
    "        if l_val is not None and r_val is not None:\n",
    "            return (l_val, r_val)\n",
    "        else:\n",
    "            return (0, 0)\n",
    "\n",
    "    df[['salary_min', 'salary_max']] = df['salary_range'].apply(\n",
    "        lambda x: pd.Series(parse_salary(x))\n",
    "    )\n",
    "    \n",
    "    print(\"Salary parsing complete\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b8ac7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def sentenceEmbedding(df, text_cols, model_name='all-MiniLM-L6-v2', device=None):\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "    \n",
    "    for col in text_cols:\n",
    "        tqdm.pandas(desc=f\"Embedding {col}\")\n",
    "        # Generate embeddings as numpy arrays\n",
    "        embeddings = df[col].progress_apply(lambda x: model.encode(str(x)) if pd.notna(x) else np.zeros(model.get_sentence_embedding_dimension()))\n",
    "        df[f\"{col}_embedding\"] = embeddings\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdpop8c06yi",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_embeddings(df, embedding_cols):\n",
    "    \"\"\"\n",
    "    Expand embedding arrays (stored as numpy arrays in cells) into separate columns.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Expanding Embeddings ---\")\n",
    "    \n",
    "    for col in embedding_cols:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Warning: {col} not found in dataframe\")\n",
    "            continue\n",
    "\n",
    "        embedding_matrix = np.vstack(df[col].values)\n",
    "        n_dims = embedding_matrix.shape[1]\n",
    "        col_names = [f\"{col}_dim_{i}\" for i in range(n_dims)]\n",
    "        \n",
    "        # df from embeddings\n",
    "        embedding_df = pd.DataFrame(embedding_matrix, columns=col_names, index=df.index)\n",
    "        df = pd.concat([df, embedding_df], axis=1)\n",
    "        df = df.drop(columns=[col])\n",
    "        \n",
    "        print(f\"Expanded {col}: {n_dims} dimensions\")\n",
    "    \n",
    "    print(\"Embedding expansion complete\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f68fcfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tfidf(\n",
    "    df, \n",
    "    text_cols, \n",
    "    word_ngrams=(1, 2), \n",
    "    char_ngrams=(3, 5),\n",
    "    min_df=3,            # ignore terms appearing in <3 docs\n",
    "    max_df=0.8,          # ignore terms appearing in >80% of docs\n",
    "    vectorizers=None     # Pre-fitted vectorizers for test data\n",
    "):\n",
    "    \"\"\"\n",
    "    Build TF-IDF features for text columns.\n",
    "    Creates both word-level and character-level n-grams.\n",
    "    \"\"\"\n",
    "    tfidf_results = {}\n",
    "    vectorizers_out = {} if vectorizers is None else vectorizers\n",
    "    is_training = vectorizers is None\n",
    "\n",
    "    for col in text_cols:\n",
    "        print(f'Processing {col}...')\n",
    "        \n",
    "        # Ensure all values are strings (should already be from null handling)\n",
    "        text_data = df[col].astype(str)\n",
    "\n",
    "        if is_training:\n",
    "            # Training: fit new vectorizers\n",
    "            word_vectorizer = TfidfVectorizer(\n",
    "                tokenizer=word_tokenize,\n",
    "                token_pattern=None,\n",
    "                ngram_range=word_ngrams,\n",
    "                max_features=2000,\n",
    "                min_df=min_df,\n",
    "                max_df=max_df\n",
    "            )\n",
    "            word_vec = word_vectorizer.fit_transform(text_data)\n",
    "\n",
    "            char_vectorizer = TfidfVectorizer(\n",
    "                analyzer='char_wb',\n",
    "                ngram_range=char_ngrams,\n",
    "                max_features=3000,\n",
    "                min_df=min_df,\n",
    "                max_df=max_df\n",
    "            )\n",
    "            char_vec = char_vectorizer.fit_transform(text_data)\n",
    "            \n",
    "            # Store vectorizers for reuse\n",
    "            vectorizers_out[f'{col}_word'] = word_vectorizer\n",
    "            vectorizers_out[f'{col}_char'] = char_vectorizer\n",
    "        else:\n",
    "            # Test: use pre-fitted vectorizers\n",
    "            word_vectorizer = vectorizers_out[f'{col}_word']\n",
    "            word_vec = word_vectorizer.transform(text_data)\n",
    "            \n",
    "            char_vectorizer = vectorizers_out[f'{col}_char']\n",
    "            char_vec = char_vectorizer.transform(text_data)\n",
    "\n",
    "        tfidf_results[col] = {\n",
    "            \"word_tfidf\": word_vec,\n",
    "            \"char_tfidf\": char_vec,\n",
    "            \"word_features\": word_vectorizer.get_feature_names_out(),\n",
    "            \"char_features\": char_vectorizer.get_feature_names_out()\n",
    "        }\n",
    "\n",
    "    if is_training:\n",
    "        print(\"TF-IDF feature extraction complete (fitted new vectorizers)\")\n",
    "    else:\n",
    "        print(\"TF-IDF feature extraction complete (used pre-fitted vectorizers)\")\n",
    "    \n",
    "    return tfidf_results, vectorizers_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03d0d1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tfidf_results(tfidf_results):\n",
    "    \"\"\"\n",
    "    Merge all TF-IDF matrices (word and char) into a single sparse matrix.\n",
    "    Returns both the sparse matrix and a DataFrame representation.\n",
    "    \"\"\"\n",
    "    all_matrices = []\n",
    "    all_feature_names = []\n",
    "\n",
    "    for col, result in tfidf_results.items():\n",
    "        word_features = [f\"{col}_word_{f}\" for f in result[\"word_features\"]]\n",
    "        all_feature_names.extend(word_features)\n",
    "        all_matrices.append(result[\"word_tfidf\"])\n",
    "        \n",
    "        char_features = [f\"{col}_char_{f}\" for f in result[\"char_features\"]]\n",
    "        all_feature_names.extend(char_features)\n",
    "        all_matrices.append(result[\"char_tfidf\"])\n",
    "\n",
    "    # Combine all sparse matrices horizontally\n",
    "    combined_matrix = hstack(all_matrices).tocsr()\n",
    "\n",
    "    # Create a sparse DataFrame (efficient for large feature sets)\n",
    "    tfidf_df = pd.DataFrame.sparse.from_spmatrix(combined_matrix, columns=all_feature_names)\n",
    "\n",
    "    print(f\"TF-IDF merge complete: {combined_matrix.shape[0]} samples × {combined_matrix.shape[1]} features\")\n",
    "    return combined_matrix, tfidf_df, all_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dnachmzkxcu",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_svd_reduction(tfidf_matrix, n_components=500, random_state=42):\n",
    "    \"\"\"\n",
    "    Apply TruncatedSVD to reduce TF-IDF dimensionality.\n",
    "    \"\"\"\n",
    "    print(f\"Applying TruncatedSVD: {tfidf_matrix.shape[1]} features -> {n_components} components\")\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=random_state)\n",
    "    svd_matrix = svd.fit_transform(tfidf_matrix)\n",
    "    return svd_matrix, svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68y1oeels9m",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_location(df):\n",
    "    def parse_location_parts(loc):\n",
    "        if not isinstance(loc, str) or loc == 'unknown':\n",
    "            return (\"unknown\", \"unknown\", \"unknown\")\n",
    "        \n",
    "        parts = [p.strip() for p in loc.split(',')]\n",
    "        while len(parts) < 3:\n",
    "            parts.append(\"unknown\")\n",
    "        return (parts[0], parts[1], parts[2])\n",
    "    \n",
    "    df_loc = df['location'].apply(parse_location_parts)\n",
    "    df['location_country'] = df_loc.apply(lambda x: x[0])\n",
    "    df['location_state'] = df_loc.apply(lambda x: x[1])\n",
    "    df['location_city'] = df_loc.apply(lambda x: x[2])\n",
    "    \n",
    "    print(\"Location parsing complete\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "555ea7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = pd.read_csv('../data/fake_job_postings.csv')\n",
    "binary_cols = [col for col in dft.columns if dft[col].nunique() == 2]\n",
    "categorical_cols = [col for col in dft.columns if 2 < dft[col].nunique() < 150]\n",
    "text_cols = [col for col in dft.columns if dft[col].dtype == 'object' and col not in categorical_cols + ['job_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c497ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(initial_df, n_svd_components=500, svd_model=None, tfidf_vectorizers=None):\n",
    "    df = initial_df.copy()\n",
    "    \n",
    "    print(f\"Starting with {len(df)} job postings\")\n",
    "    \n",
    "    # STEP 1: Handle nulls upfront\n",
    "    df = handle_nulls_upfront(df)\n",
    "    validate_nulls(df, \"initial null handling\")\n",
    "    \n",
    "    # STEP 2: Deduplicate\n",
    "    df = cleanAndDeduplicate(df)\n",
    "    validate_nulls(df, \"deduplication\")\n",
    "    \n",
    "    # STEP 3: Add pattern features (on raw text)\n",
    "    df = addWordPatterns(df)\n",
    "    validate_nulls(df, \"pattern features\")\n",
    "    \n",
    "    # STEP 4: Normalize text\n",
    "    print(\"\\n--- Text Processing ---\")\n",
    "    df = apply_text_normalization(df, TEXT_COLS)\n",
    "    \n",
    "    # STEP 5: Split joined words\n",
    "    df = apply_split_df(df, TEXT_COLS)\n",
    "    \n",
    "    # STEP 6: Generate embeddings (before stopword removal)\n",
    "    print(\"\\n--- Generating Embeddings ---\")\n",
    "    sentence_cols = ['description', 'requirements', 'benefits', 'company_profile']\n",
    "    df = sentenceEmbedding(df, text_cols=sentence_cols)\n",
    "    validate_nulls(df, \"sentence embeddings\")\n",
    "    \n",
    "    # STEP 7: Remove stopwords\n",
    "    df = remove_stopwords_df(df, TEXT_COLS)\n",
    "    \n",
    "    # STEP 8: Lemmatize\n",
    "    df = lemmatize_df(df, TEXT_COLS)\n",
    "    \n",
    "    # STEP 9: Build TF-IDF for all text columns\n",
    "    print(\"\\n--- Building TF-IDF Features ---\")\n",
    "    tfidf_results, tfidf_vectorizers = build_tfidf(df, TEXT_COLS, vectorizers=tfidf_vectorizers)\n",
    "    tfidf_matrix, tfidf_df, tfidf_feature_names = merge_tfidf_results(tfidf_results)\n",
    "    \n",
    "    # STEP 10: Apply TruncatedSVD to reduce TF-IDF dimensions\n",
    "    print(\"\\n--- Applying TruncatedSVD Dimensionality Reduction ---\")\n",
    "    if svd_model is None:\n",
    "        # Training data: fit new SVD model\n",
    "        svd_matrix, svd_model = apply_svd_reduction(tfidf_matrix, n_components=n_svd_components)\n",
    "        print(\"✓ Fitted new SVD model\")\n",
    "    else:\n",
    "        # Test data: use pre-fitted SVD model\n",
    "        svd_matrix = svd_model.transform(tfidf_matrix)\n",
    "        explained_variance = svd_model.explained_variance_ratio_.sum()\n",
    "        print(f\"✓ Used pre-fitted SVD model\")\n",
    "        print(f\"Explained variance: {explained_variance:.2%}\")\n",
    "        print(f\"SVD output shape: {svd_matrix.shape}\")\n",
    "    \n",
    "    # Convert SVD matrix to DataFrame\n",
    "    svd_col_names = [f\"tfidf_svd_{i}\" for i in range(svd_matrix.shape[1])]\n",
    "    svd_df = pd.DataFrame(svd_matrix, columns=svd_col_names, index=df.index)\n",
    "    \n",
    "    # STEP 11: Parse salary range\n",
    "    df = parse_salary_range(df)\n",
    "    validate_nulls(df, \"salary parsing\")\n",
    "    \n",
    "    # STEP 12: Parse location\n",
    "    df = parse_location(df)\n",
    "    validate_nulls(df, \"location parsing\")\n",
    "    \n",
    "    # STEP 13: Prepare final features\n",
    "    categorical_cols = [\n",
    "        'employment_type_clean', \n",
    "        'required_experience', \n",
    "        'required_education', \n",
    "        'industry', \n",
    "        'function',\n",
    "        'department',\n",
    "        'location_country', \n",
    "        'location_state', \n",
    "        'location_city'\n",
    "    ]\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "    \n",
    "    # Drop intermediate columns (processed text, original location/employment/salary)\n",
    "    df_features = df.drop(\n",
    "        columns=TEXT_COLS + ['dedup_key', 'location', 'employment_type', 'salary_range'], \n",
    "        errors='ignore'\n",
    "    )\n",
    "    \n",
    "    # STEP 14: Expand embeddings into separate columns\n",
    "    embedding_cols = [f'{col}_embedding' for col in sentence_cols]\n",
    "    df_features = expand_embeddings(df_features, embedding_cols)\n",
    "    \n",
    "    # STEP 15: Add SVD-reduced TF-IDF features\n",
    "    print(\"\\n--- Adding TF-IDF SVD Features ---\")\n",
    "    df_features = pd.concat([df_features, svd_df], axis=1)\n",
    "    print(f\"Added {svd_matrix.shape[1]} TF-IDF SVD features\")\n",
    "    \n",
    "    validate_nulls(df_features, \"final processing\")\n",
    "    \n",
    "    print(f\"\\n=== Final Feature Summary ===\")\n",
    "    print(f\"Total features: {df_features.shape[1]}\")\n",
    "    print(f\"Total samples: {df_features.shape[0]}\")\n",
    "    \n",
    "    return df_features, svd_model, tfidf_vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8c872ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 14304 job postings\n",
      "No nulls found after initial null handling\n",
      "Removed 259 duplicate rows\n",
      "No nulls found after deduplication\n",
      "Pattern features added\n",
      "No nulls found after pattern features\n",
      "\n",
      "--- Text Processing ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing title: 100%|██████████| 14045/14045 [00:00<00:00, 506731.01it/s]\n",
      "Normalizing description: 100%|██████████| 14045/14045 [00:00<00:00, 26263.87it/s]\n",
      "Normalizing requirements: 100%|██████████| 14045/14045 [00:00<00:00, 53580.28it/s]\n",
      "Normalizing benefits: 100%|██████████| 14045/14045 [00:00<00:00, 131189.05it/s]\n",
      "Normalizing company_profile: 100%|██████████| 14045/14045 [00:00<00:00, 49747.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text normalization complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting joined words in title: 100%|██████████| 14045/14045 [00:00<00:00, 89873.19it/s]\n",
      "Splitting joined words in description: 100%|██████████| 14045/14045 [00:06<00:00, 2122.37it/s]\n",
      "Splitting joined words in requirements: 100%|██████████| 14045/14045 [00:04<00:00, 3141.94it/s]\n",
      "Splitting joined words in benefits: 100%|██████████| 14045/14045 [00:01<00:00, 12976.33it/s]\n",
      "Splitting joined words in company_profile: 100%|██████████| 14045/14045 [00:02<00:00, 5121.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word splitting complete\n",
      "\n",
      "--- Generating Embeddings ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding description: 100%|██████████| 14045/14045 [02:56<00:00, 79.69it/s]\n",
      "Embedding requirements: 100%|██████████| 14045/14045 [02:03<00:00, 113.78it/s]\n",
      "Embedding benefits: 100%|██████████| 14045/14045 [01:23<00:00, 167.67it/s]\n",
      "Embedding company_profile: 100%|██████████| 14045/14045 [02:11<00:00, 106.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No nulls found after sentence embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing stopwords in title: 100%|██████████| 14045/14045 [00:00<00:00, 70395.50it/s]\n",
      "Removing stopwords in description: 100%|██████████| 14045/14045 [00:03<00:00, 4425.88it/s]\n",
      "Removing stopwords in requirements: 100%|██████████| 14045/14045 [00:01<00:00, 8883.41it/s]\n",
      "Removing stopwords in benefits: 100%|██████████| 14045/14045 [00:00<00:00, 23911.37it/s]\n",
      "Removing stopwords in company_profile: 100%|██████████| 14045/14045 [00:01<00:00, 8365.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopword removal complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lemmatizing title: 100%|██████████| 14045/14045 [00:01<00:00, 8488.60it/s] \n",
      "Lemmatizing description: 100%|██████████| 14045/14045 [00:04<00:00, 3045.10it/s]\n",
      "Lemmatizing requirements: 100%|██████████| 14045/14045 [00:02<00:00, 5902.99it/s]\n",
      "Lemmatizing benefits: 100%|██████████| 14045/14045 [00:00<00:00, 16362.78it/s]\n",
      "Lemmatizing company_profile: 100%|██████████| 14045/14045 [00:02<00:00, 6043.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization complete\n",
      "\n",
      "--- Building TF-IDF Features ---\n",
      "Processing title...\n",
      "Processing description...\n",
      "Processing requirements...\n",
      "Processing benefits...\n",
      "Processing company_profile...\n",
      "TF-IDF feature extraction complete (fitted new vectorizers)\n",
      "TF-IDF merge complete: 14045 samples × 25000 features\n",
      "\n",
      "--- Applying TruncatedSVD Dimensionality Reduction ---\n",
      "Applying TruncatedSVD: 25000 features -> 500 components\n",
      "Explained variance: 62.30%\n",
      "SVD output shape: (14045, 500)\n",
      "✓ Fitted new SVD model\n",
      "Salary parsing complete\n",
      "No nulls found after salary parsing\n",
      "Location parsing complete\n",
      "No nulls found after location parsing\n",
      "\n",
      "--- Expanding Embeddings ---\n",
      "Expanded description_embedding: 384 dimensions\n",
      "Expanded requirements_embedding: 384 dimensions\n",
      "Expanded benefits_embedding: 384 dimensions\n",
      "Expanded company_profile_embedding: 384 dimensions\n",
      "Embedding expansion complete\n",
      "\n",
      "--- Adding TF-IDF SVD Features ---\n",
      "Added 500 TF-IDF SVD features\n",
      "No nulls found after final processing\n",
      "\n",
      "=== Final Feature Summary ===\n",
      "Total features: 2072\n",
      "Total samples: 14045\n",
      "Starting with 3576 job postings\n",
      "No nulls found after initial null handling\n",
      "Removed 15 duplicate rows\n",
      "No nulls found after deduplication\n",
      "Pattern features added\n",
      "No nulls found after pattern features\n",
      "\n",
      "--- Text Processing ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing title: 100%|██████████| 3561/3561 [00:00<00:00, 446661.58it/s]\n",
      "Normalizing description: 100%|██████████| 3561/3561 [00:00<00:00, 27599.51it/s]\n",
      "Normalizing requirements: 100%|██████████| 3561/3561 [00:00<00:00, 54722.95it/s]\n",
      "Normalizing benefits: 100%|██████████| 3561/3561 [00:00<00:00, 133781.63it/s]\n",
      "Normalizing company_profile: 100%|██████████| 3561/3561 [00:00<00:00, 51895.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text normalization complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting joined words in title: 100%|██████████| 3561/3561 [00:00<00:00, 91994.29it/s]\n",
      "Splitting joined words in description: 100%|██████████| 3561/3561 [00:01<00:00, 2298.53it/s]\n",
      "Splitting joined words in requirements: 100%|██████████| 3561/3561 [00:01<00:00, 3331.45it/s]\n",
      "Splitting joined words in benefits: 100%|██████████| 3561/3561 [00:00<00:00, 13460.04it/s]\n",
      "Splitting joined words in company_profile: 100%|██████████| 3561/3561 [00:00<00:00, 5926.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word splitting complete\n",
      "\n",
      "--- Generating Embeddings ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding description: 100%|██████████| 3561/3561 [00:41<00:00, 85.85it/s] \n",
      "Embedding requirements: 100%|██████████| 3561/3561 [00:26<00:00, 132.08it/s]\n",
      "Embedding benefits: 100%|██████████| 3561/3561 [00:16<00:00, 209.87it/s]\n",
      "Embedding company_profile: 100%|██████████| 3561/3561 [00:31<00:00, 112.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No nulls found after sentence embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing stopwords in title: 100%|██████████| 3561/3561 [00:00<00:00, 55976.66it/s]\n",
      "Removing stopwords in description: 100%|██████████| 3561/3561 [00:00<00:00, 4197.27it/s]\n",
      "Removing stopwords in requirements: 100%|██████████| 3561/3561 [00:00<00:00, 8604.58it/s]\n",
      "Removing stopwords in benefits: 100%|██████████| 3561/3561 [00:00<00:00, 22313.29it/s]\n",
      "Removing stopwords in company_profile: 100%|██████████| 3561/3561 [00:00<00:00, 8250.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopword removal complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lemmatizing title: 100%|██████████| 3561/3561 [00:00<00:00, 50129.27it/s]\n",
      "Lemmatizing description: 100%|██████████| 3561/3561 [00:01<00:00, 2926.27it/s]\n",
      "Lemmatizing requirements: 100%|██████████| 3561/3561 [00:00<00:00, 5577.96it/s]\n",
      "Lemmatizing benefits: 100%|██████████| 3561/3561 [00:00<00:00, 15065.95it/s]\n",
      "Lemmatizing company_profile: 100%|██████████| 3561/3561 [00:00<00:00, 5569.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization complete\n",
      "\n",
      "--- Building TF-IDF Features ---\n",
      "Processing title...\n",
      "Processing description...\n",
      "Processing requirements...\n",
      "Processing benefits...\n",
      "Processing company_profile...\n",
      "TF-IDF feature extraction complete (used pre-fitted vectorizers)\n",
      "TF-IDF merge complete: 3561 samples × 25000 features\n",
      "\n",
      "--- Applying TruncatedSVD Dimensionality Reduction ---\n",
      "✓ Used pre-fitted SVD model\n",
      "Explained variance: 62.30%\n",
      "SVD output shape: (3561, 500)\n",
      "Salary parsing complete\n",
      "No nulls found after salary parsing\n",
      "Location parsing complete\n",
      "No nulls found after location parsing\n",
      "\n",
      "--- Expanding Embeddings ---\n",
      "Expanded description_embedding: 384 dimensions\n",
      "Expanded requirements_embedding: 384 dimensions\n",
      "Expanded benefits_embedding: 384 dimensions\n",
      "Expanded company_profile_embedding: 384 dimensions\n",
      "Embedding expansion complete\n",
      "\n",
      "--- Adding TF-IDF SVD Features ---\n",
      "Added 500 TF-IDF SVD features\n",
      "No nulls found after final processing\n",
      "\n",
      "=== Final Feature Summary ===\n",
      "Total features: 2072\n",
      "Total samples: 3561\n",
      "\n",
      "================================================================================\n",
      "FINAL SHAPES\n",
      "================================================================================\n",
      "Training set: (14045, 2072)\n",
      "Test set: (3561, 2072)\n"
     ]
    }
   ],
   "source": [
    "## For downstream notebooks, we will create train and test datasets\n",
    "\n",
    "df_raw = pd.read_csv('../data/fake_job_postings.csv')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_raw, test_raw = train_test_split(df_raw, test_size=0.2, random_state=42)\n",
    "\n",
    "# training data \n",
    "df_train, svd_model, tfidf_vectorizers = preprocess_df(train_raw, n_svd_components=500)\n",
    "\n",
    "# test data (reuses the fitted TF-IDF vectorizers and SVD model from training)\n",
    "df_test, _, _ = preprocess_df(test_raw, svd_model=svd_model, tfidf_vectorizers=tfidf_vectorizers)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL SHAPES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Training set: {df_train.shape}\")\n",
    "print(f\"Test set: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9a81831",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('../data/processed_train_features.csv', index=False)\n",
    "df_test.to_csv('../data/processed_test_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "349d492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ce4c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraudjob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
