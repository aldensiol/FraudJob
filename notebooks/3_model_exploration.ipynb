{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b494218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve,\n",
    "    confusion_matrix, classification_report, average_precision_score\n",
    ")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256d757a",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ec35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = pd.read_csv('../data/processed_train_features.csv'), pd.read_csv('../data/processed_test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9aafb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_col = 'job_id'\n",
    "target_col = 'fraudulent'\n",
    "\n",
    "X_train = df_train.drop(columns=[id_col, target_col])\n",
    "y_train = df_train[target_col]\n",
    "\n",
    "X_test = df_test.drop(columns=[id_col, target_col])\n",
    "y_test = df_test[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be91bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bf3ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded = X_train.copy()\n",
    "X_test_encoded = X_test.copy()\n",
    "\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_train_encoded[col] = le.fit_transform(X_train_encoded[col].astype(str))\n",
    "    test_values = X_test_encoded[col].astype(str)\n",
    "    # map unseen categories to a new label\n",
    "    X_test_encoded[col] = test_values.map(lambda x: le.transform([x])[0] if x in le.classes_ else -1)\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(f\"Encoded training shape: {X_train_encoded.shape}\")\n",
    "print(f\"Encoded test shape: {X_test_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z23pyp0f9e",
   "metadata": {},
   "source": [
    "## 2. Baseline Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i7xqes3gkea",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t125f22kky",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, random_state=RANDOM_STATE, eval_metric='logloss'),\n",
    "    'LightGBM': LGBMClassifier(n_estimators=100, random_state=RANDOM_STATE, verbose=-1)\n",
    "}\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    results = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "        'avg_precision': average_precision_score(y_test, y_pred_proba)\n",
    "    }\n",
    "    return results, y_pred, y_pred_proba\n",
    "\n",
    "baseline_results = {}\n",
    "baseline_predictions = {}\n",
    "\n",
    "print(\"Evaluating baseline models...\\n\")\n",
    "for name, model in tqdm(models.items()):\n",
    "    print(f\"{name}:\")\n",
    "    results, y_pred, y_pred_proba = evaluate_model(model, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "    baseline_results[name] = results\n",
    "    baseline_predictions[name] = {'pred': y_pred, 'proba': y_pred_proba}\n",
    "    print(f\"  Accuracy: {results['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {results['precision']:.4f}\")\n",
    "    print(f\"  Recall: {results['recall']:.4f}\")\n",
    "    print(f\"  F1-Score: {results['f1']:.4f}\")\n",
    "    print(f\"  ROC-AUC: {results['roc_auc']:.4f}\")\n",
    "    print(f\"  Avg Precision: {results['avg_precision']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vboo15vq9v",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df = pd.DataFrame(baseline_results).T\n",
    "baseline_df = baseline_df.sort_values('f1', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "baseline_df.plot(kind='bar', ax=ax)\n",
    "ax.set_title('Baseline Model Performance Comparison')\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Score')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_xticklabels(baseline_df.index, rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBaseline Model Rankings (by F1-Score):\")\n",
    "print(baseline_df[['f1', 'precision', 'recall', 'roc_auc']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20lvx14fxn6",
   "metadata": {},
   "source": [
    "## 3. Class Imbalance Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gqq6a3j2sks",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_techniques = {\n",
    "    'SMOTE': SMOTE(random_state=RANDOM_STATE),\n",
    "    'ADASYN': ADASYN(random_state=RANDOM_STATE),\n",
    "    'Random Undersampling': RandomUnderSampler(random_state=RANDOM_STATE),\n",
    "    'SMOTE + Tomek': SMOTETomek(random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "imbalance_results = {}\n",
    "\n",
    "for technique_name, sampler in sampling_techniques.items():\n",
    "    print(f\"\\nApplying {technique_name}...\")\n",
    "    X_resampled, y_resampled = sampler.fit_resample(X_train_scaled, y_train)\n",
    "    print(f\"  Resampled shape: {X_resampled.shape}\")\n",
    "    print(f\"  Class distribution: {pd.Series(y_resampled).value_counts().to_dict()}\")\n",
    "    \n",
    "    technique_results = {}\n",
    "    for model_name, model in models.items():\n",
    "        results, y_pred, y_pred_proba = evaluate_model(model, X_resampled, y_resampled, X_test_scaled, y_test)\n",
    "        technique_results[model_name] = results\n",
    "    \n",
    "    imbalance_results[technique_name] = technique_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s9bhh5txolt",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_combo = {}\n",
    "for technique, models_results in imbalance_results.items():\n",
    "    for model, metrics in models_results.items():\n",
    "        combo_name = f\"{technique} + {model}\"\n",
    "        best_combo[combo_name] = metrics\n",
    "\n",
    "best_combo_df = pd.DataFrame(best_combo).T\n",
    "best_combo_df = best_combo_df.sort_values('f1', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Model + Sampling Technique Combinations (by F1-Score):\")\n",
    "print(best_combo_df.head(10)[['f1', 'precision', 'recall', 'roc_auc']])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "metrics = ['f1', 'precision', 'recall', 'roc_auc']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    top_10 = best_combo_df.nlargest(10, metric)\n",
    "    ax.barh(range(len(top_10)), top_10[metric])\n",
    "    ax.set_yticks(range(len(top_10)))\n",
    "    ax.set_yticklabels(top_10.index, fontsize=8)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel(metric.upper())\n",
    "    ax.set_title(f'Top 10 Combinations by {metric.upper()}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wf8t7sxd5pk",
   "metadata": {},
   "source": [
    "## 4. Detailed Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bryzsldrr6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = best_combo_df.index[0]\n",
    "print(f\"Best Model Combination: {best_model_name}\")\n",
    "print(f\"F1-Score: {best_combo_df.iloc[0]['f1']:.4f}\")\n",
    "\n",
    "technique_name = best_model_name.split(' + ')[0]\n",
    "model_name = best_model_name.split(' + ')[1]\n",
    "\n",
    "sampler = sampling_techniques[technique_name]\n",
    "X_best_resampled, y_best_resampled = sampler.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "best_model = models[model_name]\n",
    "best_model.fit(X_best_resampled, y_best_resampled)\n",
    "\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "y_pred_proba_best = best_model.predict_proba(X_test_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8gp4em55wrv",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test, y_pred_best, target_names=['Legitimate', 'Fraudulent']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2876pnjwl",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba_best)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba_best)\n",
    "\n",
    "axes[0].plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})', linewidth=2)\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba_best)\n",
    "avg_precision = average_precision_score(y_test, y_pred_proba_best)\n",
    "\n",
    "axes[1].plot(recall, precision, label=f'PR Curve (AP = {avg_precision:.4f})', linewidth=2)\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dxi9xilun8g",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.hist(y_pred_proba_best[y_test == 0], bins=50, alpha=0.6, label='Legitimate', color='green')\n",
    "ax.hist(y_pred_proba_best[y_test == 1], bins=50, alpha=0.6, label='Fraudulent', color='red')\n",
    "ax.set_xlabel('Predicted Probability')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Prediction Probability Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cxqczm0kaaw",
   "metadata": {},
   "source": [
    "## 5. Cross-Validation on Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iqlmpyjhn1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "cv_scores = {\n",
    "    'accuracy': cross_val_score(best_model, X_best_resampled, y_best_resampled, cv=cv, scoring='accuracy'),\n",
    "    'precision': cross_val_score(best_model, X_best_resampled, y_best_resampled, cv=cv, scoring='precision'),\n",
    "    'recall': cross_val_score(best_model, X_best_resampled, y_best_resampled, cv=cv, scoring='recall'),\n",
    "    'f1': cross_val_score(best_model, X_best_resampled, y_best_resampled, cv=cv, scoring='f1'),\n",
    "    'roc_auc': cross_val_score(best_model, X_best_resampled, y_best_resampled, cv=cv, scoring='roc_auc')\n",
    "}\n",
    "\n",
    "print(\"Cross-Validation Results (5-Fold):\")\n",
    "print(\"=\" * 50)\n",
    "for metric, scores in cv_scores.items():\n",
    "    print(f\"{metric.upper():12} - Mean: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "positions = range(len(cv_scores))\n",
    "ax.boxplot(cv_scores.values(), positions=positions, labels=cv_scores.keys())\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Cross-Validation Score Distribution')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aq6u8dhi0p",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l1ok1401ima",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"MODEL EXPLORATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(f\"  Training samples: {len(X_train)}\")\n",
    "print(f\"  Test samples: {len(X_test)}\")\n",
    "print(f\"  Number of features: {X_train.shape[1]}\")\n",
    "print(f\"  Class imbalance ratio: {y_train.value_counts()[0] / y_train.value_counts()[1]:.2f}:1\")\n",
    "\n",
    "print(f\"\\nBest Model Configuration:\")\n",
    "print(f\"  Model: {model_name}\")\n",
    "print(f\"  Sampling Technique: {technique_name}\")\n",
    "print(f\"  F1-Score: {best_combo_df.iloc[0]['f1']:.4f}\")\n",
    "print(f\"  Precision: {best_combo_df.iloc[0]['precision']:.4f}\")\n",
    "print(f\"  Recall: {best_combo_df.iloc[0]['recall']:.4f}\")\n",
    "print(f\"  ROC-AUC: {best_combo_df.iloc[0]['roc_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nTop 5 Most Important Features:\")\n",
    "for idx, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"  {idx + 1}. {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\\nFeature Group Importance (Top 3):\")\n",
    "for idx, row in group_importance_df.head(3).iterrows():\n",
    "    print(f\"  {idx + 1}. {row['Feature Group']}: {row['Total Importance']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d86bfc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraudjob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
