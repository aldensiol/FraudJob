{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267488de",
   "metadata": {},
   "source": [
    "**Feature Engineering Pipeline**\n",
    "Steps:\n",
    "1. Standardize & Clean Text\n",
    "2. Stopword Removal\n",
    "3. Lemmatization\n",
    "4. Lexical Features\n",
    "5. TF-IDF (Word + Char)\n",
    "6. Embeddings (DistilBERT)\n",
    "7. Structured & Pattern Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83a79848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tingw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tingw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tingw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\tingw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# NLP preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "LEMMATIZER = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92a2be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Employment Type for deduplication\n",
    "\n",
    "def cleanAndDeduplicate(df):\n",
    "\n",
    "    df_cleaning = df.copy()\n",
    "\n",
    "    # Clean columns by replacing nulls and unspecified values with 'unknown'\n",
    "    cols_to_clean = ['required_experience', 'required_education', 'industry', 'function']\n",
    "    unspecified_values = ['Not Applicable','NaN','not applicable', 'Unspecified', 'Other','Others','none', 'na', 'n/a', '', ' ', None]\n",
    "\n",
    "    for col in cols_to_clean:\n",
    "        df_cleaning[col] = df_cleaning[col].replace(unspecified_values, 'unknown')\n",
    "        df_cleaning[col] = df_cleaning[col].fillna('unknown')\n",
    "\n",
    "    for col in df_cleaning.columns:\n",
    "        df_cleaning[col] = df_cleaning[col].fillna('unknown')\n",
    "\n",
    "    # Adding columns for location cleaning: [country, state, city]\n",
    "    def clean_location(loc):\n",
    "        if pd.isna(loc) or loc in unspecified_values:\n",
    "            return (\"unknown\", \"unknown\", \"unknown\")\n",
    "        parts = loc.split(',')\n",
    "        parts = [part.strip() if part.strip() not in unspecified_values else \"unknown\" for part in parts]\n",
    "        # Pad with \"unknown\" if we don't have all three parts\n",
    "        while len(parts) < 3:\n",
    "            parts.append(\"unknown\")\n",
    "        return (parts[0], parts[1], parts[2])\n",
    "    df_cleaning_loc = df_cleaning['location'].apply(clean_location)\n",
    "    df_cleaning['location_country'] = df_cleaning_loc.apply(lambda x: x[0])\n",
    "    df_cleaning['location_state'] = df_cleaning_loc.apply(lambda x: x[1])\n",
    "    df_cleaning['location_city'] = df_cleaning_loc.apply(lambda x: x[2])\n",
    "\n",
    "    def simplify_employment_type(x):\n",
    "        if pd.isna(x):\n",
    "            return 'unknown'\n",
    "        x = x.strip().lower()\n",
    "        if x in ['full-time', 'part-time']:\n",
    "            return x  # keep these separate\n",
    "        elif x in ['contract', 'temporary']:\n",
    "            return 'non-permanent'\n",
    "        elif x in ['other', 'unknown', '']:\n",
    "            return 'unknown'\n",
    "        else:\n",
    "            return 'unknown'\n",
    "    df_cleaning['employment_type_clean'] = df_cleaning['employment_type'].apply(simplify_employment_type)\n",
    "\n",
    "    # Define a new function to compare locations accounting for unknowns\n",
    "    def compare_locations(row1, row2):\n",
    "        # Compare countries first\n",
    "        if row1['location_country'] is None or row2['location_country'] is None:\n",
    "            return True\n",
    "        if row1['location_country'] != row2['location_country']:\n",
    "            return False\n",
    "        # If countries match, compare states (unless either is unknown)\n",
    "        if (row1['location_state'] != 'unknown' and \n",
    "            row2['location_state'] != 'unknown' and \n",
    "            row1['location_state'] != row2['location_state']):\n",
    "            return False\n",
    "        # If states match or either is unknown, compare cities (unless either is unknown)\n",
    "        if (row1['location_city'] != 'unknown' and \n",
    "            row2['location_city'] != 'unknown' and \n",
    "            row1['location_city'] != row2['location_city']):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    # Create a new comparison key function that uses the location comparison\n",
    "    def comparison_key(row):\n",
    "        # Get location info with handling for unknown values\n",
    "        location_info = (\n",
    "            row['location_country'],\n",
    "            row['location_state'] if row['location_state'] != 'unknown' else None,\n",
    "            row['location_city'] if row['location_city'] != 'unknown' else None\n",
    "        )\n",
    "        \n",
    "        # Get employment type, None if unknown\n",
    "        emp = None if row['employment_type_clean'] == 'unknown' else row['employment_type_clean']\n",
    "        \n",
    "        # Return tuple with all comparison fields\n",
    "        return (location_info, row['title'], row['description'], row['requirements'], emp)\n",
    "\n",
    "    df_cleaning['dedup_key'] = df_cleaning.apply(comparison_key, axis=1)\n",
    "    df = df_cleaning.drop_duplicates(subset=['dedup_key'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0633e526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "\n",
    "def check_corpus(df, text_cols, top_n=50):\n",
    "    corpus_stats = {}\n",
    "    \n",
    "    for col in text_cols:\n",
    "        tqdm.pandas(desc=f\"Analyzing {col}\")\n",
    "        \n",
    "        # Combine all text in the column\n",
    "        all_text = df[col].fillna(\"\").astype(str).progress_apply(lambda x: x.lower())\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = [token for text in all_text for token in word_tokenize(text) if len(token) > 2]\n",
    "        \n",
    "        # Count\n",
    "        counts = Counter(tokens)\n",
    "        \n",
    "        # Most common\n",
    "        corpus_stats[col] = counts.most_common(top_n)\n",
    "    \n",
    "    print(f\"Corpus size (number of unique tokens): {len(set(tokens))}\")\n",
    "    \n",
    "    return corpus_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ed2de51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing title: 100%|██████████| 17880/17880 [00:00<00:00, 1655025.17it/s]\n",
      "Analyzing description: 100%|██████████| 17880/17880 [00:00<00:00, 155398.03it/s]\n",
      "Analyzing requirements: 100%|██████████| 17880/17880 [00:00<00:00, 315402.32it/s]\n",
      "Analyzing benefits: 100%|██████████| 17880/17880 [00:00<00:00, 587935.84it/s]\n",
      "Analyzing company_profile: 100%|██████████| 17880/17880 [00:00<00:00, 283695.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size (number of unique tokens): 18468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing title: 100%|██████████| 17880/17880 [00:00<00:00, 275735.10it/s]\n",
      "Normalizing description: 100%|██████████| 17880/17880 [00:01<00:00, 15752.99it/s]\n",
      "Normalizing requirements: 100%|██████████| 17880/17880 [00:00<00:00, 33603.84it/s]\n",
      "Normalizing benefits: 100%|██████████| 17880/17880 [00:00<00:00, 82402.65it/s]\n",
      "Normalizing company_profile: 100%|██████████| 17880/17880 [00:00<00:00, 30031.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# 1. Standardize and Clean\n",
    "def normalize_text(text: str) -> str:\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # remove URLs\n",
    "    text = re.sub(r\"[^a-z\\s']\", \" \", text)  # remove punctuation/numbers except apostrophes\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def apply_text_normalization(df, text_cols):\n",
    "    \n",
    "    for col in text_cols:\n",
    "        tqdm.pandas(desc=f\"Normalizing {col}\")\n",
    "        df[col] = df[col].progress_apply(normalize_text)\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv('../fake_job_postings.csv')\n",
    "\n",
    "raw_corpus = check_corpus(df, ['title', 'description', 'requirements', 'benefits', 'company_profile'], top_n=50)\n",
    "\n",
    "df = apply_text_normalization(df, ['title', 'description', 'requirements', 'benefits', 'company_profile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5831e226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing stopwords in title: 100%|██████████| 17880/17880 [00:00<00:00, 45628.37it/s]\n",
      "Removing stopwords in description: 100%|██████████| 17880/17880 [00:06<00:00, 2624.27it/s]\n",
      "Removing stopwords in requirements: 100%|██████████| 17880/17880 [00:03<00:00, 5342.96it/s]\n",
      "Removing stopwords in benefits: 100%|██████████| 17880/17880 [00:01<00:00, 14042.50it/s]\n",
      "Removing stopwords in company_profile: 100%|██████████| 17880/17880 [00:03<00:00, 4948.84it/s]\n",
      "Lemmatizing title: 100%|██████████| 17880/17880 [00:00<00:00, 33611.47it/s]\n",
      "Lemmatizing description: 100%|██████████| 17880/17880 [00:09<00:00, 1970.79it/s]\n",
      "Lemmatizing requirements: 100%|██████████| 17880/17880 [00:04<00:00, 3866.94it/s]\n",
      "Lemmatizing benefits: 100%|██████████| 17880/17880 [00:01<00:00, 10569.49it/s]\n",
      "Lemmatizing company_profile: 100%|██████████| 17880/17880 [00:04<00:00, 3763.36it/s]\n",
      "Analyzing title: 100%|██████████| 17880/17880 [00:00<00:00, 1730967.24it/s]\n",
      "Analyzing description: 100%|██████████| 17880/17880 [00:00<00:00, 921734.42it/s]\n",
      "Analyzing requirements: 100%|██████████| 17880/17880 [00:00<00:00, 1119967.68it/s]\n",
      "Analyzing benefits: 100%|██████████| 17880/17880 [00:00<00:00, 1688525.14it/s]\n",
      "Analyzing company_profile: 100%|██████████| 17880/17880 [00:00<00:00, 1306791.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size (number of unique tokens): 12650\n"
     ]
    }
   ],
   "source": [
    "# 2. Remove stopwords\n",
    "def remove_stopwords_df(df, text_cols):\n",
    "    \"\"\"\n",
    "    Remove English stopwords from multiple text columns in a DataFrame.\n",
    "    \"\"\"\n",
    "    def remove_stopwords_text(text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        clean_tokens = [t for t in tokens if t not in STOPWORDS and len(t) > 2]\n",
    "        return \" \".join(clean_tokens)\n",
    "    \n",
    "    for col in text_cols:\n",
    "        tqdm.pandas(desc=f\"Removing stopwords in {col}\")\n",
    "        df[col] = df[col].progress_apply(remove_stopwords_text)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = remove_stopwords_df(df, ['title', 'description', 'requirements', 'benefits', 'company_profile'])\n",
    "\n",
    "\n",
    "def lemmatize_df(df, text_cols):\n",
    "    \"\"\"\n",
    "    Lemmatize multiple text columns in a DataFrame.\n",
    "    \"\"\"\n",
    "    def lemmatize_text(text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        lemmas = [lemmatizer.lemmatize(t) for t in tokens if len(t) > 2]\n",
    "        return \" \".join(lemmas)\n",
    "    \n",
    "    for col in text_cols:\n",
    "        tqdm.pandas(desc=f\"Lemmatizing {col}\")\n",
    "        df[col] = df[col].progress_apply(lemmatize_text)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = lemmatize_df(df, ['title', 'description', 'requirements', 'benefits', 'company_profile'])\n",
    "\n",
    "cleaned_corpus = check_corpus(df, ['title', 'description', 'requirements', 'benefits', 'company_profile'], top_n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24e4c07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATTERN FEATURES\n",
    "\n",
    "def addWordPatterns(df):    \n",
    "    patterns = {\n",
    "        'urls': r'http[s]?://\\S+',\n",
    "        'emails': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
    "        'phone_numbers': r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b',\n",
    "        'money_symbols': r'[$€£¥]',\n",
    "        'other_symbols': r'[©®™]',\n",
    "    }\n",
    "    \n",
    "    def count_patterns(text, patterns):\n",
    "        # Ensure text is a string\n",
    "        if not isinstance(text, str):\n",
    "            text = \"\"\n",
    "        return {k: len(re.findall(pat, text)) for k, pat in patterns.items()}\n",
    "\n",
    "    for col in ['description', 'company_profile', 'requirements', 'benefits']:\n",
    "        feat_df = pd.DataFrame(list(df[col].apply(count_patterns)), index=df.index).add_prefix(f'{col}_')\n",
    "        df = pd.concat([df, feat_df], axis=1)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebe82074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding Categorical Variables\n",
    "def one_hot_encode(df, categorical_cols):\n",
    "    df = df.copy()\n",
    "    ohe = pd.get_dummies(df[categorical_cols].fillna('unknown'), prefix=categorical_cols)\n",
    "    return pd.concat([df, ohe], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f68fcfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. TF-IDF Feature Extraction\n",
    "\n",
    "def build_tfidf(df, text_cols, word_ngrams=(1, 2), char_ngrams=(3, 5), max_features=2000):\n",
    "    tfidf_blocks = []\n",
    "    word_vectorizer = TfidfVectorizer(ngram_range=word_ngrams, max_features=max_features)\n",
    "    char_vectorizer = TfidfVectorizer(analyzer='char', ngram_range=char_ngrams, max_features=max_features)\n",
    "\n",
    "    for col in text_cols:\n",
    "        word_vec = word_vectorizer.fit_transform(df[col].fillna(''))\n",
    "        char_vec = char_vectorizer.fit_transform(df[col].fillna(''))\n",
    "        tfidf_blocks.extend([word_vec, char_vec])\n",
    "\n",
    "    combined = hstack(tfidf_blocks)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d28d40c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # optional: shows progress bar for large DataFrames\n",
    "\n",
    "def sentenceEmbedding(df, text_col='text_column', device=None):\n",
    "    # Load DistilBERT tokenizer & model once\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    # Use GPU if available, I have AMD GPU :(\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()  # set to inference mode\n",
    "\n",
    "    def get_embedding(text):\n",
    "        if pd.isna(text) or not isinstance(text, str) or text.strip() == \"\":\n",
    "            return torch.zeros(model.config.hidden_size, device=device)\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "        return embeddings\n",
    "\n",
    "    tqdm.pandas(desc=\"Generating embeddings\")  # progress bar\n",
    "    df[\"text_embedding\"] = df[text_col].progress_apply(get_embedding)\n",
    "\n",
    "    # Optional: convert to list for storage or export\n",
    "    df[\"text_embedding_list\"] = df[\"text_embedding\"].apply(lambda x: x.cpu().tolist())\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83148dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08f45791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suscpicious word counts for job descriptions/company (using heuristics)\n",
    "\n",
    "def addSuspiciousWordCount(df):\n",
    "\n",
    "    suspicious_keywords = [\n",
    "        'quick money', 'easy money', 'high pay', 'earn fast', 'upfront fee',\n",
    "        'no experience required', 'work from home', 'bonus', 'registration fee',\n",
    "        'processing fee', 'pay before start', 'investment required', 'fee upfront',\n",
    "        'deposit required', 'apply now', 'limited spots', 'immediate start',\n",
    "        'act fast', 'urgent', 'deadline', 'be your own boss', 'flexible hours',\n",
    "        'online opportunity', 'training provided', 'click here', 'get rich'\n",
    "    ]\n",
    "    def count_suspicious_words(text, keywords):\n",
    "        text_lower = text.lower()\n",
    "        return sum(text_lower.count(keyword) for keyword in keywords)  \n",
    "    # df['description_suspicious_word_count'] = df['description'].apply(lambda x: count_suspicious_words(x, suspicious_keywords))\n",
    "    # df['company_profile_suspicious_word_count'] = df['company_profile'].apply(lambda x: count_suspicious_words(x, suspicious_keywords))\n",
    "    description_suspicious_word_count = df['description'].apply(lambda x: count_suspicious_words(x, suspicious_keywords))\n",
    "    company_profile_suspicious_word_count = df['company_profile'].apply(lambda x: count_suspicious_words(x, suspicious_keywords))\n",
    "    benefits_suspicious_word_count = df['benefits'].apply(lambda x: count_suspicious_words(x, suspicious_keywords))\n",
    "    requirements_suspicious_word_count = df['requirements'].apply(lambda x: count_suspicious_words(x, suspicious_keywords))\n",
    "\n",
    "    df['description_suspicious_word_count'] = description_suspicious_word_count\n",
    "    df['company_profile_suspicious_word_count'] = company_profile_suspicious_word_count\n",
    "    df['benefits_suspicious_word_count'] = benefits_suspicious_word_count\n",
    "    df['requirements_suspicious_word_count'] = requirements_suspicious_word_count\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7a66056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for categorical variables\n",
    "\n",
    "def oneHotEncoding(df, categorical_cols):\n",
    "    ohe = pd.get_dummies(df[categorical_cols].fillna('unknown').astype(str), prefix=categorical_cols, prefix_sep='__', dummy_na=False)\n",
    "    df = pd.concat([df, ohe], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5e64832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse Salary\n",
    "def parse_salary_range(df):\n",
    "    MONTH_MAP = {\n",
    "    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,\n",
    "    'jul': 7, 'aug': 8, 'sep': 9, 'sept': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "}   \n",
    "    def parse_salary(s):\n",
    "        if pd.isna(s) or str(s).lower() == 'unknown':\n",
    "            return (None, None)\n",
    "\n",
    "        s = str(s).strip()\n",
    "        if '-' not in s:\n",
    "            try: return (int(s), int(s))\n",
    "            except ValueError: return (None, None)\n",
    "\n",
    "        left, right = [v.strip() for v in s.split('-', 1)]\n",
    "\n",
    "        def val(v):\n",
    "            return int(v) if v.isdigit() else MONTH_MAP.get(v.lower())\n",
    "\n",
    "        l_val, r_val = val(left), val(right)\n",
    "        return (l_val, r_val) if l_val is not None and r_val is not None else (None, None)\n",
    "\n",
    "    df[['salary_min', 'salary_max']] = df['salary_range'].apply(lambda x: pd.Series(parse_salary(x)))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8556e960",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "addWordPatterns.<locals>.count_patterns() missing 1 required positional argument: 'patterns'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m text_cols = [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df.columns \u001b[38;5;28;01mif\u001b[39;00m df[col].dtype == \u001b[33m'\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m categorical_cols + [\u001b[33m'\u001b[39m\u001b[33mjob_id\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m      8\u001b[39m df = cleanAndDeduplicate(df)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m df = \u001b[43maddWordPatterns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m df = addSuspiciousWordCount(df)\n\u001b[32m     11\u001b[39m df = oneHotEncoding(df, categorical_cols)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36maddWordPatterns\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[38;5;28mlen\u001b[39m(re.findall(pat, text)) \u001b[38;5;28;01mfor\u001b[39;00m k, pat \u001b[38;5;129;01min\u001b[39;00m patterns.items()}\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcompany_profile\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrequirements\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mbenefits\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     feat_df = pd.DataFrame(\u001b[38;5;28mlist\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcount_patterns\u001b[49m\u001b[43m)\u001b[49m), index=df.index).add_prefix(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     20\u001b[39m     df = pd.concat([df, feat_df], axis=\u001b[32m1\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\series.py:4943\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4808\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4810\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4815\u001b[39m     **kwargs,\n\u001b[32m   4816\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4817\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4818\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4819\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4934\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4935\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4936\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4937\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4939\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4941\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: addWordPatterns.<locals>.count_patterns() missing 1 required positional argument: 'patterns'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../fake_job_postings.csv')\n",
    "\n",
    "# Colum types via EDA\n",
    "binary_cols = [col for col in df.columns if df[col].nunique() == 2]\n",
    "categorical_cols = [col for col in df.columns if 2 < df[col].nunique() < 150]\n",
    "text_cols = [col for col in df.columns if df[col].dtype == 'object' and col not in categorical_cols + ['job_id']]\n",
    "\n",
    "df = cleanAndDeduplicate(df)\n",
    "df = addWordPatterns(df)\n",
    "df = addSuspiciousWordCount(df)\n",
    "df = oneHotEncoding(df, categorical_cols)\n",
    "df = parse_salary_range(df)\n",
    "\n",
    "# TDIDF\n",
    "WORD_NGRAM = (1, 2)\n",
    "CHAR_NGRAM = (3, 5)\n",
    "MAX_FEATURES = 1000\n",
    "\n",
    "word_tfidf_matrix = addAllWordTDIDF(df, WORD_NGRAM, MAX_FEATURES)\n",
    "char_tfidf_matrix = addAllCharTDIDF(df, CHAR_NGRAM, MAX_FEATURES)\n",
    "combined_tfidf = hstack([word_tfidf_matrix, char_tfidf_matrix])\n",
    "\n",
    "# Sentence Embedding\n",
    "df = sentenceEmbedding(df, text_col='description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8d2d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   job_id  telecommuting  has_company_logo  has_questions  fraudulent  \\\n",
      "0       1              0                 1              0           0   \n",
      "1       2              0                 1              0           0   \n",
      "2       3              0                 1              0           0   \n",
      "3       4              0                 1              0           0   \n",
      "4       5              0                 1              1           0   \n",
      "\n",
      "  location_country location_state location_city employment_type_clean  \\\n",
      "0               US             NY      New York               unknown   \n",
      "1               NZ        unknown      Auckland             full-time   \n",
      "2               US             IA         Wever               unknown   \n",
      "3               US             DC    Washington             full-time   \n",
      "4               US             FL    Fort Worth             full-time   \n",
      "\n",
      "                                           dedup_key  ...  tfidf_9990  \\\n",
      "0  ((US, NY, New York), Marketing Intern, Food52,...  ...    0.000000   \n",
      "1  ((NZ, None, Auckland), Customer Service - Clou...  ...    0.015412   \n",
      "2  ((US, IA, Wever), Commissioning Machinery Assi...  ...    0.000000   \n",
      "3  ((US, DC, Washington), Account Executive - Was...  ...    0.000000   \n",
      "4  ((US, FL, Fort Worth), Bill Review Manager, JO...  ...    0.000000   \n",
      "\n",
      "   tfidf_9991  tfidf_9992  tfidf_9993  tfidf_9994  tfidf_9995  tfidf_9996  \\\n",
      "0    0.000000     0.00000         0.0    0.000000    0.000000    0.000000   \n",
      "1    0.015587     0.01526         0.0    0.200269    0.134055    0.049947   \n",
      "2    0.000000     0.00000         0.0    0.000000    0.000000    0.000000   \n",
      "3    0.000000     0.00000         0.0    0.000000    0.000000    0.000000   \n",
      "4    0.000000     0.00000         0.0    0.000000    0.000000    0.000000   \n",
      "\n",
      "   tfidf_9997  tfidf_9998  tfidf_9999  \n",
      "0    0.000000    0.000000    0.000000  \n",
      "1    0.082428    0.069443    0.033721  \n",
      "2    0.000000    0.000000    0.000000  \n",
      "3    0.000000    0.000000    0.000000  \n",
      "4    0.000000    0.000000    0.000000  \n",
      "\n",
      "[5 rows x 10233 columns]\n",
      "New df shape: (17479, 10233)\n"
     ]
    }
   ],
   "source": [
    "# Drop Raw Columns\n",
    "to_drop = categorical_cols + text_cols\n",
    "df_dropped = df.drop(columns=to_drop)\n",
    "\n",
    "# Combine\n",
    "tfidf_feature_names = [f\"tfidf_{i}\" for i in range(combined_tfidf.shape[1])]\n",
    "tfidf_df = pd.DataFrame(combined_tfidf.toarray(), columns=tfidf_feature_names, index=df_dropped.index)\n",
    "df_combined = pd.concat([df_dropped, tfidf_df], axis=1)\n",
    "\n",
    "print(df_combined.head())\n",
    "print(\"New df shape:\", df_combined.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
