{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83a79848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5831e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Employment Type for deduplication\n",
    "\n",
    "def cleanAndDeduplicate(df):\n",
    "\n",
    "    df_cleaning = df.copy()\n",
    "\n",
    "    # Clean columns by replacing nulls and unspecified values with 'unknown'\n",
    "    cols_to_clean = ['required_experience', 'required_education', 'industry', 'function']\n",
    "    unspecified_values = ['Not Applicable','NaN','not applicable', 'Unspecified', 'Other','Others','none', 'na', 'n/a', '', ' ', None]\n",
    "\n",
    "    for col in cols_to_clean:\n",
    "        df_cleaning[col] = df_cleaning[col].replace(unspecified_values, 'unknown')\n",
    "        df_cleaning[col] = df_cleaning[col].fillna('unknown')\n",
    "\n",
    "    for col in df_cleaning.columns:\n",
    "        df_cleaning[col] = df_cleaning[col].fillna('unknown')\n",
    "\n",
    "    # Adding columns for location cleaning: [country, state, city]\n",
    "    def clean_location(loc):\n",
    "        if pd.isna(loc) or loc in unspecified_values:\n",
    "            return (\"unknown\", \"unknown\", \"unknown\")\n",
    "        parts = loc.split(',')\n",
    "        parts = [part.strip() if part.strip() not in unspecified_values else \"unknown\" for part in parts]\n",
    "        # Pad with \"unknown\" if we don't have all three parts\n",
    "        while len(parts) < 3:\n",
    "            parts.append(\"unknown\")\n",
    "        return (parts[0], parts[1], parts[2])\n",
    "    df_cleaning_loc = df_cleaning['location'].apply(clean_location)\n",
    "    df_cleaning['location_country'] = df_cleaning_loc.apply(lambda x: x[0])\n",
    "    df_cleaning['location_state'] = df_cleaning_loc.apply(lambda x: x[1])\n",
    "    df_cleaning['location_city'] = df_cleaning_loc.apply(lambda x: x[2])\n",
    "\n",
    "    def simplify_employment_type(x):\n",
    "        if pd.isna(x):\n",
    "            return 'unknown'\n",
    "        x = x.strip().lower()\n",
    "        if x in ['full-time', 'part-time']:\n",
    "            return x  # keep these separate\n",
    "        elif x in ['contract', 'temporary']:\n",
    "            return 'non-permanent'\n",
    "        elif x in ['other', 'unknown', '']:\n",
    "            return 'unknown'\n",
    "        else:\n",
    "            return 'unknown'\n",
    "    df_cleaning['employment_type_clean'] = df_cleaning['employment_type'].apply(simplify_employment_type)\n",
    "\n",
    "    # Define a new function to compare locations accounting for unknowns\n",
    "    def compare_locations(row1, row2):\n",
    "        # Compare countries first\n",
    "        if row1['location_country'] is None or row2['location_country'] is None:\n",
    "            return True\n",
    "        if row1['location_country'] != row2['location_country']:\n",
    "            return False\n",
    "        # If countries match, compare states (unless either is unknown)\n",
    "        if (row1['location_state'] != 'unknown' and \n",
    "            row2['location_state'] != 'unknown' and \n",
    "            row1['location_state'] != row2['location_state']):\n",
    "            return False\n",
    "        # If states match or either is unknown, compare cities (unless either is unknown)\n",
    "        if (row1['location_city'] != 'unknown' and \n",
    "            row2['location_city'] != 'unknown' and \n",
    "            row1['location_city'] != row2['location_city']):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    # Create a new comparison key function that uses the location comparison\n",
    "    def comparison_key(row):\n",
    "        # Get location info with handling for unknown values\n",
    "        location_info = (\n",
    "            row['location_country'],\n",
    "            row['location_state'] if row['location_state'] != 'unknown' else None,\n",
    "            row['location_city'] if row['location_city'] != 'unknown' else None\n",
    "        )\n",
    "        \n",
    "        # Get employment type, None if unknown\n",
    "        emp = None if row['employment_type_clean'] == 'unknown' else row['employment_type_clean']\n",
    "        \n",
    "        # Return tuple with all comparison fields\n",
    "        return (location_info, row['title'], row['description'], row['requirements'], emp)\n",
    "\n",
    "    df_cleaning['dedup_key'] = df_cleaning.apply(comparison_key, axis=1)\n",
    "    df = df_cleaning.drop_duplicates(subset=['dedup_key'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2caa7583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate based on all columns except 'job_id'\n",
    "# Checked in EDA, okay to drop duplicates this way\n",
    "\n",
    "\n",
    "#Unused\n",
    "def deduplicate(df):\n",
    "    df_dedup = df.drop_duplicates(subset=[col for col in df.columns if col != 'job_id'])\n",
    "    print(f\"Original shape: {df.shape}, Deduplicated shape: {df_dedup.shape}\")\n",
    "    return df_dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f68fcfb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# word n-grams (1-2)\\nword_tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=1000, lowercase=True)\\n\\n# char n-grams (3-5)\\nchar_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3,5), max_features=1000)\\n\\n# Fit & transform per field\\ntitle_word_tfidf = word_tfidf.fit_transform(df['title'])\\ncompany_profile_word_tfidf = word_tfidf.fit_transform(df['company_profile'])\\nderscription_word_tfidf = word_tfidf.fit_transform(df['description'])\\nrequirements_word_tfidf = word_tfidf.fit_transform(df['requirements'])\\nbenefits_word_tfidf = word_tfidf.fit_transform(df['benefits'])\\n\\ntitle_char_tfidf = char_tfidf.fit_transform(df['title'])\\ncompany_profile_char_tfidf = char_tfidf.fit_transform(df['company_profile'])\\nderscription_char_tfidf = char_tfidf.fit_transform(df['description'])\\nrequirements_char_tfidf = char_tfidf.fit_transform(df['requirements'])\\nbenefits_char_tfidf = char_tfidf.fit_transform(df['benefits'])\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "def wordTDIDFfunc(col, ngram_range, max_features):\n",
    "    word_tfidf = TfidfVectorizer(ngram_range=ngram_range, max_features=max_features, lowercase=True)\n",
    "    return word_tfidf.fit_transform(col)\n",
    "\n",
    "\n",
    "def charTDIDFfunc(col, ngram_range, max_features):\n",
    "    char_tfidf = TfidfVectorizer(analyzer='char', ngram_range=ngram_range, max_features=max_features)\n",
    "    return char_tfidf.fit_transform(col)\n",
    "\n",
    "def addAllWordTDIDF(df, ngram_range, max_features, text_cols=['title', 'company_profile', 'description', 'requirements', 'benefits']):\n",
    "    tfidf_features = []\n",
    "    for col in text_cols:\n",
    "        X_col = wordTDIDFfunc(df[col], ngram_range, max_features)\n",
    "        tfidf_features.append(X_col)\n",
    "    return hstack(tfidf_features)\n",
    "\n",
    "\n",
    "def addAllCharTDIDF(df, ngram_range, max_features, text_cols=['title', 'company_profile', 'description', 'requirements', 'benefits']):\n",
    "    tfidf_features = []\n",
    "    for col in text_cols:\n",
    "        X_col = charTDIDFfunc(df[col], ngram_range, max_features)\n",
    "        tfidf_features.append(X_col)\n",
    "    return hstack(tfidf_features)\n",
    "    \n",
    "\"\"\"# word n-grams (1-2)\n",
    "word_tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=1000, lowercase=True)\n",
    "\n",
    "# char n-grams (3-5)\n",
    "char_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3,5), max_features=1000)\n",
    "\n",
    "# Fit & transform per field\n",
    "title_word_tfidf = word_tfidf.fit_transform(df['title'])\n",
    "company_profile_word_tfidf = word_tfidf.fit_transform(df['company_profile'])\n",
    "derscription_word_tfidf = word_tfidf.fit_transform(df['description'])\n",
    "requirements_word_tfidf = word_tfidf.fit_transform(df['requirements'])\n",
    "benefits_word_tfidf = word_tfidf.fit_transform(df['benefits'])\n",
    "\n",
    "title_char_tfidf = char_tfidf.fit_transform(df['title'])\n",
    "company_profile_char_tfidf = char_tfidf.fit_transform(df['company_profile'])\n",
    "derscription_char_tfidf = char_tfidf.fit_transform(df['description'])\n",
    "requirements_char_tfidf = char_tfidf.fit_transform(df['requirements'])\n",
    "benefits_char_tfidf = char_tfidf.fit_transform(df['benefits'])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28d40c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # optional: shows progress bar for large DataFrames\n",
    "\n",
    "def sentenceEmbedding(df, text_col='text_column', device=None):\n",
    "    # Load DistilBERT tokenizer & model once\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    # Use GPU if available, I have AMD GPU :(\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()  # set to inference mode\n",
    "\n",
    "    def get_embedding(text):\n",
    "        if pd.isna(text) or not isinstance(text, str) or text.strip() == \"\":\n",
    "            return torch.zeros(model.config.hidden_size, device=device)\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "        return embeddings\n",
    "\n",
    "    tqdm.pandas(desc=\"Generating embeddings\")  # progress bar\n",
    "    df[\"text_embedding\"] = df[text_col].progress_apply(get_embedding)\n",
    "\n",
    "    # Optional: convert to list for storage or export\n",
    "    df[\"text_embedding_list\"] = df[\"text_embedding\"].apply(lambda x: x.cpu().tolist())\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83148dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of text tfidf (URLs, email, phone numbers, money symbols, other symbols, etc)\n",
    "# Columns: 'description', 'company_profile', 'requirements', 'benefits'\n",
    "import re\n",
    "\n",
    "def addWordPatterns(df):\n",
    "    \n",
    "    patterns = {\n",
    "        'urls': r'http[s]?://\\S+',\n",
    "        'emails': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
    "        'phone_numbers': r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b',\n",
    "        'money_symbols': r'[$€£¥]',\n",
    "        'other_symbols': r'[©®™]',\n",
    "    }\n",
    "    \n",
    "    def count_patterns(text, patterns):\n",
    "        # Ensure text is a string\n",
    "        if not isinstance(text, str):\n",
    "            text = \"\"\n",
    "        counts = {}\n",
    "        for name, pattern in patterns.items():\n",
    "            counts[name] = len(re.findall(pattern, text))\n",
    "        return counts\n",
    "\n",
    "    # build and join prefixed char-feature DataFrames for each text field\n",
    "    desc_char_tfidf = pd.DataFrame(\n",
    "        list(df['description'].apply(lambda x: count_patterns(x, patterns))),\n",
    "        index=df.index\n",
    "    ).add_prefix('description_').fillna(0).astype(int)\n",
    "    df = pd.concat([df, desc_char_tfidf], axis=1)\n",
    "\n",
    "    company_profile_char_tfidf = pd.DataFrame(\n",
    "        list(df['company_profile'].apply(lambda x: count_patterns(x, patterns))),\n",
    "        index=df.index\n",
    "    ).add_prefix('company_profile_').fillna(0).astype(int)\n",
    "    df = pd.concat([df, company_profile_char_tfidf], axis=1)\n",
    "\n",
    "    requirements_char_tfidf = pd.DataFrame(\n",
    "        list(df['requirements'].apply(lambda x: count_patterns(x, patterns))),\n",
    "        index=df.index\n",
    "    ).add_prefix('requirements_').fillna(0).astype(int)\n",
    "    df = pd.concat([df, requirements_char_tfidf], axis=1)\n",
    "\n",
    "    benefits_char_tfidf = pd.DataFrame(\n",
    "        list(df['benefits'].apply(lambda x: count_patterns(x, patterns))),\n",
    "        index=df.index\n",
    "    ).add_prefix('benefits_').fillna(0).astype(int)\n",
    "    df = pd.concat([df, benefits_char_tfidf], axis=1)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08f45791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suscpicious word counts for job descriptions/company (using heuristics)\n",
    "\n",
    "def addSuspiciousWordCount(df):\n",
    "\n",
    "    suspicious_keywords = [\n",
    "        'quick money', 'easy money', 'high pay', 'earn fast', 'upfront fee',\n",
    "        'no experience required', 'work from home', 'bonus', 'registration fee',\n",
    "        'processing fee', 'pay before start', 'investment required', 'fee upfront',\n",
    "        'deposit required', 'apply now', 'limited spots', 'immediate start',\n",
    "        'act fast', 'urgent', 'deadline', 'be your own boss', 'flexible hours',\n",
    "        'online opportunity', 'training provided', 'click here', 'get rich'\n",
    "    ]\n",
    "    def count_suspicious_words(text, keywords):\n",
    "        text_lower = text.lower()\n",
    "        return sum(text_lower.count(keyword) for keyword in keywords)  \n",
    "    # df['description_suspicious_word_count'] = df['description'].apply(lambda x: count_suspicious_words(x, suspicious_keywords))\n",
    "    # df['company_profile_suspicious_word_count'] = df['company_profile'].apply(lambda x: count_suspicious_words(x, suspicious_keywords))\n",
    "    description_suspicious_word_count = df['description'].apply(lambda x: count_suspicious_words(x, suspicious_keywords))\n",
    "    company_profile_suspicious_word_count = df['company_profile'].apply(lambda x: count_suspicious_words(x, suspicious_keywords))\n",
    "    benefits_suspicious_word_count = df['benefits'].apply(lambda x: count_suspicious_words(x, suspicious_keywords))\n",
    "    requirements_suspicious_word_count = df['requirements'].apply(lambda x: count_suspicious_words(x, suspicious_keywords))\n",
    "\n",
    "    df['description_suspicious_word_count'] = description_suspicious_word_count\n",
    "    df['company_profile_suspicious_word_count'] = company_profile_suspicious_word_count\n",
    "    df['benefits_suspicious_word_count'] = benefits_suspicious_word_count\n",
    "    df['requirements_suspicious_word_count'] = requirements_suspicious_word_count\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7a66056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for categorical variables\n",
    "\n",
    "def oneHotEncoding(df, categorical_cols):\n",
    "    ohe = pd.get_dummies(df[categorical_cols].fillna('unknown').astype(str), prefix=categorical_cols, prefix_sep='__', dummy_na=False)\n",
    "    df = pd.concat([df, ohe], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5e64832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse Salary\n",
    "def parse_salary_range(df):\n",
    "    MONTH_MAP = {\n",
    "    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,\n",
    "    'jul': 7, 'aug': 8, 'sep': 9, 'sept': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "}   \n",
    "    def parse_salary(s):\n",
    "        if pd.isna(s) or str(s).lower() == 'unknown':\n",
    "            return (None, None)\n",
    "\n",
    "        s = str(s).strip()\n",
    "        if '-' not in s:\n",
    "            try: return (int(s), int(s))\n",
    "            except ValueError: return (None, None)\n",
    "\n",
    "        left, right = [v.strip() for v in s.split('-', 1)]\n",
    "\n",
    "        def val(v):\n",
    "            return int(v) if v.isdigit() else MONTH_MAP.get(v.lower())\n",
    "\n",
    "        l_val, r_val = val(left), val(right)\n",
    "        return (l_val, r_val) if l_val is not None and r_val is not None else (None, None)\n",
    "\n",
    "    df[['salary_min', 'salary_max']] = df['salary_range'].apply(lambda x: pd.Series(parse_salary(x)))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8556e960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 17479/17479 [23:54<00:00, 12.19it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../fake_job_postings.csv')\n",
    "\n",
    "# Colum types via EDA\n",
    "binary_cols = [col for col in df.columns if df[col].nunique() == 2]\n",
    "categorical_cols = [col for col in df.columns if 2 < df[col].nunique() < 150]\n",
    "text_cols = [col for col in df.columns if df[col].dtype == 'object' and col not in categorical_cols + ['job_id']]\n",
    "\n",
    "df = cleanAndDeduplicate(df)\n",
    "df = addWordPatterns(df)\n",
    "df = addSuspiciousWordCount(df)\n",
    "df = oneHotEncoding(df, categorical_cols)\n",
    "df = parse_salary_range(df)\n",
    "\n",
    "# TDIDF\n",
    "WORD_NGRAM = (1, 2)\n",
    "CHAR_NGRAM = (3, 5)\n",
    "MAX_FEATURES = 1000\n",
    "\n",
    "word_tfidf_matrix = addAllWordTDIDF(df, WORD_NGRAM, MAX_FEATURES)\n",
    "char_tfidf_matrix = addAllCharTDIDF(df, CHAR_NGRAM, MAX_FEATURES)\n",
    "combined_tfidf = hstack([word_tfidf_matrix, char_tfidf_matrix])\n",
    "\n",
    "# Sentence Embedding\n",
    "df = sentenceEmbedding(df, text_col='description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c8d2d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   job_id  telecommuting  has_company_logo  has_questions  fraudulent  \\\n",
      "0       1              0                 1              0           0   \n",
      "1       2              0                 1              0           0   \n",
      "2       3              0                 1              0           0   \n",
      "3       4              0                 1              0           0   \n",
      "4       5              0                 1              1           0   \n",
      "\n",
      "  location_country location_state location_city employment_type_clean  \\\n",
      "0               US             NY      New York               unknown   \n",
      "1               NZ        unknown      Auckland             full-time   \n",
      "2               US             IA         Wever               unknown   \n",
      "3               US             DC    Washington             full-time   \n",
      "4               US             FL    Fort Worth             full-time   \n",
      "\n",
      "                                           dedup_key  ...  tfidf_9990  \\\n",
      "0  ((US, NY, New York), Marketing Intern, Food52,...  ...    0.000000   \n",
      "1  ((NZ, None, Auckland), Customer Service - Clou...  ...    0.015412   \n",
      "2  ((US, IA, Wever), Commissioning Machinery Assi...  ...    0.000000   \n",
      "3  ((US, DC, Washington), Account Executive - Was...  ...    0.000000   \n",
      "4  ((US, FL, Fort Worth), Bill Review Manager, JO...  ...    0.000000   \n",
      "\n",
      "   tfidf_9991  tfidf_9992  tfidf_9993  tfidf_9994  tfidf_9995  tfidf_9996  \\\n",
      "0    0.000000     0.00000         0.0    0.000000    0.000000    0.000000   \n",
      "1    0.015587     0.01526         0.0    0.200269    0.134055    0.049947   \n",
      "2    0.000000     0.00000         0.0    0.000000    0.000000    0.000000   \n",
      "3    0.000000     0.00000         0.0    0.000000    0.000000    0.000000   \n",
      "4    0.000000     0.00000         0.0    0.000000    0.000000    0.000000   \n",
      "\n",
      "   tfidf_9997  tfidf_9998  tfidf_9999  \n",
      "0    0.000000    0.000000    0.000000  \n",
      "1    0.082428    0.069443    0.033721  \n",
      "2    0.000000    0.000000    0.000000  \n",
      "3    0.000000    0.000000    0.000000  \n",
      "4    0.000000    0.000000    0.000000  \n",
      "\n",
      "[5 rows x 10233 columns]\n",
      "New df shape: (17479, 10233)\n"
     ]
    }
   ],
   "source": [
    "# Drop Raw Columns\n",
    "to_drop = categorical_cols + text_cols\n",
    "df_dropped = df.drop(columns=to_drop)\n",
    "\n",
    "# Combine\n",
    "tfidf_feature_names = [f\"tfidf_{i}\" for i in range(combined_tfidf.shape[1])]\n",
    "tfidf_df = pd.DataFrame(combined_tfidf.toarray(), columns=tfidf_feature_names, index=df_dropped.index)\n",
    "df_combined = pd.concat([df_dropped, tfidf_df], axis=1)\n",
    "\n",
    "print(df_combined.head())\n",
    "print(\"New df shape:\", df_combined.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
