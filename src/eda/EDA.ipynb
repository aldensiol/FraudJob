{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba9ab76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../fake_job_postings.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5fec87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: title, Unique values: 11231\n",
      "Column: location, Unique values: 3105\n",
      "Column: department, Unique values: 1337\n",
      "Column: salary_range, Unique values: 874\n",
      "Column: company_profile, Unique values: 1709\n",
      "Column: description, Unique values: 14801\n",
      "Column: requirements, Unique values: 11967\n",
      "Column: benefits, Unique values: 6204\n",
      "Column: employment_type, Unique values: 5\n",
      "Column: required_experience, Unique values: 7\n",
      "Column: required_education, Unique values: 13\n",
      "Column: industry, Unique values: 131\n",
      "Column: function, Unique values: 37\n",
      "Binary columns: ['telecommuting', 'has_company_logo', 'has_questions', 'fraudulent']\n",
      "Categorical columns: ['employment_type', 'required_experience', 'required_education', 'industry', 'function']\n",
      "Text columns: ['title', 'location', 'department', 'salary_range', 'company_profile', 'description', 'requirements', 'benefits']\n"
     ]
    }
   ],
   "source": [
    "binary_cols = [col for col in df.columns if df[col].nunique() == 2]\n",
    "raw_text_cols = [col for col in df.columns if df[col].dtype == 'object' and col not in binary_cols + ['job_id']]\n",
    "\n",
    "# print each count of unique value each in text columns\n",
    "for col in raw_text_cols:\n",
    "    print(f\"Column: {col}, Unique values: {df[col].nunique()}\")\n",
    "\n",
    "# categorical columns (less than 150 unique values)\n",
    "categorical_cols = [col for col in df.columns if 2 < df[col].nunique() < 150]\n",
    "text_cols = [col for col in df.columns if df[col].dtype == 'object' and col not in categorical_cols + ['job_id']]\n",
    "\n",
    "# print columns\n",
    "print(f\"Binary columns: {binary_cols}\")\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "print(f\"Text columns: {text_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ca8f7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraudulent               0      1  Total\n",
      "salary_range         84.45  74.25  83.96\n",
      "department           64.75  61.32  64.58\n",
      "required_education   44.99  52.08  45.33\n",
      "benefits             40.25  42.03  40.34\n",
      "required_experience  38.88  50.23  39.43\n",
      "function             35.96  38.91  36.10\n",
      "industry             27.20  31.76  27.42\n",
      "employment_type      18.98  27.83  19.41\n",
      "company_profile      15.99  67.78  18.50\n",
      "requirements         14.94  17.78  15.08\n",
      "location              1.92   2.19   1.94\n",
      "description           0.00   0.12   0.01\n",
      "job_id                0.00   0.00   0.00\n",
      "telecommuting         0.00   0.00   0.00\n",
      "has_questions         0.00   0.00   0.00\n",
      "has_company_logo      0.00   0.00   0.00\n",
      "title                 0.00   0.00   0.00\n",
      "fraudulent            0.00   0.00   0.00\n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "grouped_null_percentage = df.groupby('fraudulent').apply(lambda x: x.isnull().mean() * 100)\n",
    "grouped_null_percentage = grouped_null_percentage.T.round(2)\n",
    "grouped_null_percentage['Total'] = (df.isnull().mean() * 100).round(2)\n",
    "grouped_null_percentage = grouped_null_percentage.sort_values(by='Total', ascending=False)\n",
    "print(grouped_null_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94150417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (17880, 18), Deduplicated shape: (17599, 18)\n",
      "\n",
      "=== Before dedup ===\n",
      "            count  percentage\n",
      "fraudulent                   \n",
      "0           17014       95.16\n",
      "1             866        4.84\n",
      "\n",
      "=== After dedup ===\n",
      "            count  percentage\n",
      "fraudulent                   \n",
      "0           16743       95.14\n",
      "1             856        4.86\n",
      "Column 'title' has 6368 duplicate non-null values after deduplication.\n",
      "Column 'location' has 14153 duplicate non-null values after deduplication.\n",
      "Column 'department' has 4934 duplicate non-null values after deduplication.\n",
      "Column 'salary_range' has 1953 duplicate non-null values after deduplication.\n",
      "Column 'company_profile' has 12608 duplicate non-null values after deduplication.\n",
      "Column 'description' has 2797 duplicate non-null values after deduplication.\n",
      "Column 'requirements' has 2983 duplicate non-null values after deduplication.\n",
      "Column 'benefits' has 4287 duplicate non-null values after deduplication.\n"
     ]
    }
   ],
   "source": [
    "# Deduplicate based on all columns except 'job_id'\n",
    "df_dedup = df.drop_duplicates(subset=[col for col in df.columns if col != 'job_id'])\n",
    "print(f\"Original shape: {df.shape}, Deduplicated shape: {df_dedup.shape}\")\n",
    "\n",
    "# Check class balance before and after deduplication\n",
    "for name, data in [('Before dedup', df), ('After dedup', df_dedup)]:\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    counts = data['fraudulent'].value_counts()\n",
    "    percentages = data['fraudulent'].value_counts(normalize=True) * 100\n",
    "    print(pd.DataFrame({'count': counts, 'percentage': percentages.round(2)}))\n",
    "\n",
    "# Check for duplicates in text columns after deduping\n",
    "for col in text_cols:\n",
    "    # Only consider non-null values for duplicates\n",
    "    non_null_mask = df_dedup[col].notna()\n",
    "    num_dupes = df_dedup[non_null_mask].duplicated(subset=[col]).sum()\n",
    "    print(f\"Column '{col}' has {num_dupes} duplicate non-null values after deduplication.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8303a51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 527 duplicated rows (ignoring job_id and fraudulent).\n",
      "Number of groups with inconsistent 'fraudulent' labels: 0\n"
     ]
    }
   ],
   "source": [
    "# Columns to consider for identifying duplicates (ignore 'job_id' and 'fraudulent')\n",
    "cols_to_check = df.columns.difference(['job_id', 'fraudulent'])\n",
    "\n",
    "# Identify duplicates based on all columns except 'job_id' and 'fraudulent'\n",
    "dup_mask = df.duplicated(subset=cols_to_check, keep=False)\n",
    "df_duplicates = df[dup_mask].sort_values(by=list(cols_to_check))\n",
    "print(f\"Found {df_duplicates.shape[0]} duplicated rows (ignoring job_id and fraudulent).\")\n",
    "\n",
    "# Check if duplicated rows have consistent 'fraudulent' labels\n",
    "inconsistent_labels = df.groupby(list(cols_to_check))['fraudulent'].nunique()\n",
    "inconsistent_count = (inconsistent_labels > 1).sum()\n",
    "print(f\"Number of groups with inconsistent 'fraudulent' labels: {inconsistent_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38cf525c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location\n",
      "0        94\n",
      "2     17320\n",
      "3       104\n",
      "4         7\n",
      "5         4\n",
      "6         2\n",
      "8         1\n",
      "10        1\n",
      "15        1\n",
      "Name: count, dtype: int64\n",
      "13181    US, CA, Los Angeles, California, San Francisco...\n",
      "Name: location, dtype: object\n",
      "=== Comma Count Distribution by Fraudulent Class ===\n",
      "comma_count  0      2   3   4   5   6   8   10  15\n",
      "fraudulent                                        \n",
      "0            86  16492  95   7   4   2   0   0   1\n",
      "1             8    828   9   0   0   0   1   1   0\n"
     ]
    }
   ],
   "source": [
    "# EDA for location column\n",
    "# Focus on non-null locations\n",
    "non_null_locations = df['location'].dropna()\n",
    "comma_counts = non_null_locations.str.count(',')\n",
    "location_structure = comma_counts.value_counts().sort_index()\n",
    "print(location_structure)\n",
    "\n",
    "non_standard_locations = non_null_locations[comma_counts == 15]\n",
    "print(non_standard_locations.sample(1, random_state=42))\n",
    "\n",
    "# Drop null locations first\n",
    "non_null_df = df.dropna(subset=['location']).copy()\n",
    "\n",
    "# Count commas in each location\n",
    "non_null_df['comma_count'] = non_null_df['location'].str.count(',')\n",
    "\n",
    "# Count how many entries have 0, 1, 2, 3+ commas per class\n",
    "location_structure_by_class = (\n",
    "    non_null_df.groupby('fraudulent')['comma_count']\n",
    "    .value_counts()\n",
    "    .unstack(fill_value=0)\n",
    "    .sort_index(axis=1)\n",
    ")\n",
    "\n",
    "print(\"=== Comma Count Distribution by Fraudulent Class ===\")\n",
    "print(location_structure_by_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e73512",
   "metadata": {},
   "source": [
    "Presense of duplicated rows ignoring job_id & target label, duplicated rows all have the same target label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602e2a7d",
   "metadata": {},
   "source": [
    "## Check for duplication based on partial matching\n",
    "\n",
    "### Unknown replacement for selected columns\n",
    "NA values -> 'unknown'\n",
    "Certain columns have other \"NA\" like values: eg. Not Applicable, NaN, Unspecified. They have all been replaced with 'unknown'\n",
    "\n",
    "### Location seperation/cleaning\n",
    "- Check if non-null locations are seperated by ','\n",
    "- Split based on ','\n",
    "\n",
    "### Interested Columns\n",
    "only location, employment_type, title, description and requirements are used to find duplicates.\n",
    "\n",
    "### Chunk Below\n",
    "- Cleans columns ['required_experience', 'required_education', 'industry', 'function'] by replacing N/A values with str(\"unknown\")\n",
    "- Splits column ['location'] into 3 columns ['location_country', 'location_state', 'location_city']\n",
    "- Handles employment type by:\n",
    "    - Combining 'other' & 'unknown'\n",
    "    - Combining 'temporary' & 'contract'\n",
    "- Removes duplicates with enhanced restrictions on employment type & location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdaa8e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Employment Type for deduplication\n",
    "df_cleaning = df_dedup.copy()\n",
    "\n",
    "# Clean columns by replacing nulls and unspecified values with 'unknown'\n",
    "cols_to_clean = ['required_experience', 'required_education', 'industry', 'function']\n",
    "unspecified_values = ['Not Applicable','NaN','not applicable', 'Unspecified', 'Other','Others','none', 'na', 'n/a', '', ' ', None]\n",
    "\n",
    "for col in cols_to_clean:\n",
    "    df_cleaning[col] = df_cleaning[col].replace(unspecified_values, 'unknown')\n",
    "    df_cleaning[col] = df_cleaning[col].fillna('unknown')\n",
    "\n",
    "for col in df_cleaning.columns:\n",
    "    df_cleaning[col] = df_cleaning[col].fillna('unknown')\n",
    "\n",
    "# Adding columns for location cleaning: [country, state, city]\n",
    "def clean_location(loc):\n",
    "    if pd.isna(loc) or loc in unspecified_values:\n",
    "        return (\"unknown\", \"unknown\", \"unknown\")\n",
    "    parts = loc.split(',')\n",
    "    parts = [part.strip() if part.strip() not in unspecified_values else \"unknown\" for part in parts]\n",
    "    # Pad with \"unknown\" if we don't have all three parts\n",
    "    while len(parts) < 3:\n",
    "        parts.append(\"unknown\")\n",
    "    return (parts[0], parts[1], parts[2])\n",
    "\n",
    "df_cleaning_loc = df_cleaning['location'].apply(clean_location)\n",
    "df_cleaning['location_country'] = df_cleaning_loc.apply(lambda x: x[0])\n",
    "df_cleaning['location_state'] = df_cleaning_loc.apply(lambda x: x[1])\n",
    "df_cleaning['location_city'] = df_cleaning_loc.apply(lambda x: x[2])\n",
    "\n",
    "def simplify_employment_type(x):\n",
    "    if pd.isna(x):\n",
    "        return 'unknown'\n",
    "    x = x.strip().lower()\n",
    "    if x in ['full-time', 'part-time']:\n",
    "        return x  # keep these separate\n",
    "    elif x in ['contract', 'temporary']:\n",
    "        return 'non-permanent'\n",
    "    elif x in ['other', 'unknown', '']:\n",
    "        return 'unknown'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "df_cleaning['employment_type_clean'] = df_cleaning['employment_type'].apply(simplify_employment_type)\n",
    "\n",
    "# Define a new function to compare locations accounting for unknowns\n",
    "def compare_locations(row1, row2):\n",
    "    # Compare countries first\n",
    "    if row1['location_country'] is None or row2['location_country'] is None:\n",
    "        return True\n",
    "    if row1['location_country'] != row2['location_country']:\n",
    "        return False\n",
    "    # If countries match, compare states (unless either is unknown)\n",
    "    if (row1['location_state'] != 'unknown' and \n",
    "        row2['location_state'] != 'unknown' and \n",
    "        row1['location_state'] != row2['location_state']):\n",
    "        return False\n",
    "    # If states match or either is unknown, compare cities (unless either is unknown)\n",
    "    if (row1['location_city'] != 'unknown' and \n",
    "        row2['location_city'] != 'unknown' and \n",
    "        row1['location_city'] != row2['location_city']):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Create a new comparison key function that uses the location comparison\n",
    "def comparison_key(row):\n",
    "    # Get location info with handling for unknown values\n",
    "    location_info = (\n",
    "        row['location_country'],\n",
    "        row['location_state'] if row['location_state'] != 'unknown' else None,\n",
    "        row['location_city'] if row['location_city'] != 'unknown' else None\n",
    "    )\n",
    "    \n",
    "    # Get employment type, None if unknown\n",
    "    emp = None if row['employment_type_clean'] == 'unknown' else row['employment_type_clean']\n",
    "    \n",
    "    # Return tuple with all comparison fields\n",
    "    return (location_info, row['title'], row['description'], row['requirements'], emp)\n",
    "\n",
    "df_cleaning['dedup_key'] = df_cleaning.apply(comparison_key, axis=1)\n",
    "df_nodup = df_cleaning.drop_duplicates(subset=['dedup_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61b26114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  telecommuting  has_company_logo  has_questions  fraudulent\n",
      "telecommuting          1.000000         -0.019836       0.020345    0.034523\n",
      "has_company_logo      -0.019836          1.000000       0.233932   -0.261971\n",
      "has_questions          0.020345          0.233932       1.000000   -0.091627\n",
      "fraudulent             0.034523         -0.261971      -0.091627    1.000000\n"
     ]
    }
   ],
   "source": [
    "# Binary check\n",
    "binary_cols = [col for col in df.columns if df[col].nunique() == 2]\n",
    "df_binary = df[binary_cols]\n",
    "df_binary = df_binary.apply(lambda x: x.map({'Yes': 1, 'No': 0, True: 1, False: 0}) \n",
    "                            if x.dtypes == 'object' else x)\n",
    "corr_matrix = df_binary.corr()\n",
    "print(corr_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
